{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBIsYvsYrTJz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel, CLIPTextModel\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weKmAISHGT1m",
        "outputId": "ce68cd9f-6832-4db0-83bb-26eb5154ff34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CONFIGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyGCtPjNE_5w"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Central configuration class for all hyperparameters and paths.\"\"\"\n",
        "\n",
        "    # Model\n",
        "    model_name: str = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "    # Dataset paths\n",
        "    dataset_path: str = \"/content/drive/MyDrive/GD-VCR-Dataset/questions.jsonl\"\n",
        "    image_base_path: str = \"/content/drive/MyDrive/GD-VCR-Dataset/\"\n",
        "\n",
        "    # VPT Hyperparameters\n",
        "    num_prompt_tokens: int = 50\n",
        "    prompt_dim: int = 1024\n",
        "    prompt_dropout: float = 0.1\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    batch_size: int = 16\n",
        "    learning_rate: float = 1e-3\n",
        "    num_epochs: int = 10\n",
        "    warmup_steps: int = 100\n",
        "    weight_decay: float = 0.01\n",
        "\n",
        "    # System\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed: int = 42\n",
        "    num_workers: int = 2\n",
        "\n",
        "    # Validation split\n",
        "    val_split: float = 0.3\n",
        "    max_text_length: int = 77\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # Best practice: deterministic mode for reproducibility\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hkv7pyamFG1Q"
      },
      "outputs": [],
      "source": [
        "def load_jsonl(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Load a JSONL file and return a list of dictionaries.\"\"\"\n",
        "    data = []\n",
        "    print(f\"Loading dataset from: {file_path}\")\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    data.append(record)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Warning: Could not parse line {line_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    print(f\"Loaded {len(data)} records from {file_path}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def validate_record(record: Dict) -> bool:\n",
        "    \"\"\"Validate that a GD-VCR record has all required fields.\"\"\"\n",
        "    required_fields = [\"img_fn\", \"objects\", \"question\", \"answer_choices\", \"answer_label\"]\n",
        "\n",
        "    for field in required_fields:\n",
        "        if field not in record:\n",
        "            return False\n",
        "\n",
        "    if record[\"answer_label\"] < 0 or record[\"answer_label\"] >= len(record[\"answer_choices\"]):\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Mt8HYJnFKmB"
      },
      "outputs": [],
      "source": [
        "def resolve_object_reference(token: Any, objects: List[str]) -> str:\n",
        "    \"\"\"Resolve an object reference token to its actual object name.\"\"\"\n",
        "    if isinstance(token, list) and len(token) == 1 and isinstance(token[0], int):\n",
        "        index = token[0]\n",
        "        if 0 <= index < len(objects):\n",
        "            return objects[index]\n",
        "        else:\n",
        "            return f\"object{index}\"\n",
        "\n",
        "    if isinstance(token, int):\n",
        "        if 0 <= token < len(objects):\n",
        "            return objects[token]\n",
        "        else:\n",
        "            return f\"object{token}\"\n",
        "\n",
        "    return str(token)\n",
        "\n",
        "\n",
        "def tokens_to_sentence(tokens: List[Any], objects: List[str]) -> str:\n",
        "    \"\"\"Convert a list of tokens (with object references) into a natural sentence.\"\"\"\n",
        "    resolved_tokens = [resolve_object_reference(token, objects) for token in tokens]\n",
        "    sentence = \" \".join(resolved_tokens)\n",
        "\n",
        "    punctuation = [\".\", \",\", \"!\", \"?\", \"'\", '\"', \":\", \";\"]\n",
        "    for punct in punctuation:\n",
        "        sentence = sentence.replace(f\" {punct}\", punct)\n",
        "\n",
        "    sentence = sentence.replace(\" ' s\", \"'s\")\n",
        "    sentence = sentence.replace(\" 's\", \"'s\")\n",
        "    sentence = \" \".join(sentence.split())\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def preprocess_sample(record: Dict) -> Dict:\n",
        "    \"\"\"Preprocess a single GD-VCR record into a format suitable for CLIP.\"\"\"\n",
        "    objects = record[\"objects\"]\n",
        "    question = tokens_to_sentence(record[\"question\"], objects)\n",
        "\n",
        "    answer_choices = []\n",
        "    for choice_tokens in record[\"answer_choices\"]:\n",
        "        choice_sentence = tokens_to_sentence(choice_tokens, objects)\n",
        "        answer_choices.append(choice_sentence)\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer_choices\": answer_choices,\n",
        "        \"answer_label\": record[\"answer_label\"],\n",
        "        \"img_fn\": record[\"img_fn\"],\n",
        "        \"objects\": objects,\n",
        "    }\n",
        "\n",
        "\n",
        "def create_qa_text(question: str, answer: str) -> str:\n",
        "    \"\"\"Combine question and answer into a single text for CLIP encoding.\"\"\"\n",
        "    return f\"Question: {question} Answer: {answer}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoRWRBLBFUfr"
      },
      "outputs": [],
      "source": [
        "class GDVCRDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for GD-VCR visual commonsense reasoning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        jsonl_path: str,\n",
        "        processor: CLIPProcessor,\n",
        "        config: Config,\n",
        "        image_base_path: Optional[str] = None,\n",
        "    ):\n",
        "        self.processor = processor\n",
        "        self.config = config\n",
        "        self.image_base_path = image_base_path or config.image_base_path\n",
        "\n",
        "        raw_data = load_jsonl(jsonl_path)\n",
        "\n",
        "        self.data = []\n",
        "        for record in raw_data:\n",
        "            if validate_record(record):\n",
        "                preprocessed = preprocess_sample(record)\n",
        "                self.data.append(preprocessed)\n",
        "            else:\n",
        "                print(f\"Warning: Skipping invalid record\")\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.data)} valid samples\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def load_image(self, img_fn: str) -> Image.Image:\n",
        "        \"\"\"Load an image from disk.\"\"\"\n",
        "        img_path = os.path.join(self.image_base_path, img_fn)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            return image\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image not found: {img_path}\")\n",
        "            return Image.new(\"RGB\", (224, 224), color=\"gray\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error loading {img_path}: {e}\")\n",
        "            return Image.new(\"RGB\", (224, 224), color=\"gray\")\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        image = self.load_image(sample[\"img_fn\"])\n",
        "        image_inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = image_inputs[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        question = sample[\"question\"]\n",
        "        answer_choices = sample[\"answer_choices\"]\n",
        "\n",
        "        qa_texts = []\n",
        "        for answer in answer_choices:\n",
        "            qa_text = create_qa_text(question, answer)\n",
        "            qa_texts.append(qa_text)\n",
        "\n",
        "        text_inputs = self.processor(\n",
        "            text=qa_texts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.config.max_text_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"input_ids\": text_inputs[\"input_ids\"],\n",
        "            \"attention_mask\": text_inputs[\"attention_mask\"],\n",
        "            \"label\": torch.tensor(sample[\"answer_label\"], dtype=torch.long),\n",
        "            \"num_choices\": len(answer_choices),\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Custom collate function for batching GD-VCR samples with variable choice counts.\"\"\"\n",
        "    batch_size = len(batch)\n",
        "    max_choices = max(sample[\"num_choices\"] for sample in batch)\n",
        "    max_length = batch[0][\"input_ids\"].shape[1]\n",
        "\n",
        "    pixel_values = torch.stack([sample[\"pixel_values\"] for sample in batch])\n",
        "    labels = torch.stack([sample[\"label\"] for sample in batch])\n",
        "    num_choices = torch.tensor([sample[\"num_choices\"] for sample in batch])\n",
        "\n",
        "    input_ids = torch.zeros(batch_size, max_choices, max_length, dtype=torch.long)\n",
        "    attention_mask = torch.zeros(batch_size, max_choices, max_length, dtype=torch.long)\n",
        "\n",
        "    for i, sample in enumerate(batch):\n",
        "        n_choices = sample[\"num_choices\"]\n",
        "        input_ids[i, :n_choices] = sample[\"input_ids\"]\n",
        "        attention_mask[i, :n_choices] = sample[\"attention_mask\"]\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"num_choices\": num_choices,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Baseline CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCLapraWFYN_"
      },
      "outputs": [],
      "source": [
        "class BaselineCLIP(nn.Module):\n",
        "    \"\"\"Baseline CLIP model for visual commonsense reasoning (zero-shot, all frozen).\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"Loading CLIP model: {config.model_name}\")\n",
        "        self.clip = CLIPModel.from_pretrained(config.model_name)\n",
        "\n",
        "        # Freeze all parameters\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.config = config\n",
        "        self.to(config.device)\n",
        "\n",
        "        print(f\"Baseline CLIP loaded with {self.count_parameters():,} parameters (all frozen)\")\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def count_trainable_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def encode_image(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        image_outputs = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "\n",
        "        if hasattr(image_outputs, \"pooler_output\"):\n",
        "            image_features = image_outputs.pooler_output\n",
        "        else:\n",
        "            image_features = image_outputs\n",
        "\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "        return image_features\n",
        "\n",
        "    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, num_choices, seq_len = input_ids.shape\n",
        "\n",
        "        input_ids_flat = input_ids.view(-1, seq_len)\n",
        "        attention_mask_flat = attention_mask.view(-1, seq_len)\n",
        "\n",
        "        text_outputs = self.clip.get_text_features(\n",
        "            input_ids=input_ids_flat, attention_mask=attention_mask_flat\n",
        "        )\n",
        "\n",
        "        if hasattr(text_outputs, \"pooler_output\"):\n",
        "            text_features = text_outputs.pooler_output\n",
        "        else:\n",
        "            text_features = text_outputs\n",
        "\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "        text_features = text_features.view(batch_size, num_choices, -1)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.Tensor,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        **kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        image_features = self.encode_image(pixel_values)\n",
        "        text_features = self.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        logits = torch.einsum(\"bcd,bcd->bc\", image_features.expand_as(text_features), text_features)\n",
        "\n",
        "        logit_scale = self.clip.logit_scale.exp()\n",
        "        logits = logits * logit_scale\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# VPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAmzgeu_Fgb1"
      },
      "outputs": [],
      "source": [
        "class VisualPromptTokens(nn.Module):\n",
        "    \"\"\"Learnable visual prompt tokens for VPT (prepended to image patch sequence).\"\"\"\n",
        "\n",
        "    def __init__(self, num_prompts: int, hidden_dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_prompts = num_prompts\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Learnable prompts - only trainable parameters\n",
        "        self.prompts = nn.Parameter(torch.zeros(1, num_prompts, hidden_dim))\n",
        "\n",
        "        # Best practice: Xavier initialization for proper gradient flow\n",
        "        nn.init.xavier_uniform_(self.prompts)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        print(f\"Created {num_prompts} visual prompts with dim {hidden_dim}\")\n",
        "        print(f\"Prompt parameters: {num_prompts * hidden_dim:,}\")\n",
        "\n",
        "    def forward(self, batch_size: int) -> torch.Tensor:\n",
        "        prompts = self.prompts.expand(batch_size, -1, -1)\n",
        "        prompts = self.dropout(prompts)\n",
        "        return prompts\n",
        "\n",
        "\n",
        "class CLIPWithVPT(nn.Module):\n",
        "    \"\"\"CLIP model enhanced with Visual Prompt Tuning (VPT-Shallow).\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        print(f\"Loading CLIP model for VPT: {config.model_name}\")\n",
        "        self.clip = CLIPModel.from_pretrained(config.model_name)\n",
        "\n",
        "        # Freeze CLIP backbone - only prompts are trainable\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.visual_prompts = VisualPromptTokens(\n",
        "            num_prompts=config.num_prompt_tokens,\n",
        "            hidden_dim=config.prompt_dim,\n",
        "            dropout=config.prompt_dropout,\n",
        "        )\n",
        "\n",
        "        self.to(config.device)\n",
        "\n",
        "        total_params = self.count_parameters()\n",
        "        trainable_params = self.count_trainable_parameters()\n",
        "        print(f\"Total parameters: {total_params:,}\")\n",
        "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"Trainable ratio: {100*trainable_params/total_params:.4f}%\")\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def count_trainable_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def get_visual_embeddings_with_prompts(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get visual embeddings with prompts injected after [CLS] token.\"\"\"\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        vision_model = self.clip.vision_model\n",
        "\n",
        "        embeddings = vision_model.embeddings(pixel_values)\n",
        "\n",
        "        cls_token = embeddings[:, :1, :]\n",
        "        patch_embeddings = embeddings[:, 1:, :]\n",
        "\n",
        "        prompts = self.visual_prompts(batch_size)\n",
        "\n",
        "        # Inject prompts: [CLS] + [Prompts] + [Patches]\n",
        "        embeddings_with_prompts = torch.cat([cls_token, prompts, patch_embeddings], dim=1)\n",
        "\n",
        "        hidden_states = embeddings_with_prompts\n",
        "        hidden_states = vision_model.pre_layrnorm(hidden_states)\n",
        "\n",
        "        encoder_outputs = vision_model.encoder(\n",
        "            inputs_embeds=hidden_states,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "        )\n",
        "\n",
        "        if isinstance(encoder_outputs, tuple):\n",
        "            hidden_states = encoder_outputs[0]\n",
        "        else:\n",
        "            hidden_states = encoder_outputs.last_hidden_state\n",
        "\n",
        "        pooled_output = hidden_states[:, 0, :]\n",
        "        pooled_output = vision_model.post_layernorm(pooled_output)\n",
        "\n",
        "        image_features = self.clip.visual_projection(pooled_output)\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "\n",
        "        return image_features\n",
        "\n",
        "    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, num_choices, seq_len = input_ids.shape\n",
        "\n",
        "        input_ids_flat = input_ids.view(-1, seq_len)\n",
        "        attention_mask_flat = attention_mask.view(-1, seq_len)\n",
        "\n",
        "        text_output = self.clip.get_text_features(\n",
        "            input_ids=input_ids_flat, attention_mask=attention_mask_flat\n",
        "        )\n",
        "\n",
        "        if hasattr(text_output, \"pooler_output\"):\n",
        "            text_features = text_output.pooler_output\n",
        "        elif hasattr(text_output, \"last_hidden_state\"):\n",
        "            text_features = text_output.last_hidden_state[:, 0, :]\n",
        "        elif isinstance(text_output, torch.Tensor):\n",
        "            text_features = text_output\n",
        "        else:\n",
        "            text_features = text_output[0] if isinstance(text_output, tuple) else text_output\n",
        "\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "        text_features = text_features.view(batch_size, num_choices, -1)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.Tensor,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        **kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        image_features = self.get_visual_embeddings_with_prompts(pixel_values)\n",
        "        text_features = self.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        logits = torch.einsum(\"bcd,bcd->bc\", image_features.expand_as(text_features), text_features)\n",
        "\n",
        "        logit_scale = self.clip.logit_scale.exp()\n",
        "        logits = logits * logit_scale\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkiXgu7qFm7b"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Training manager for VPT models.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        config: Config,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "\n",
        "        # Only optimize trainable parameters\n",
        "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "        self.optimizer = AdamW(trainable_params, lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        total_steps = len(train_loader) * config.num_epochs\n",
        "\n",
        "        def lr_lambda(step):\n",
        "            if step < config.warmup_steps:\n",
        "                return float(step) / float(max(1, config.warmup_steps))\n",
        "            return 1.0\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.best_val_accuracy = 0.0\n",
        "\n",
        "        print(f\"Trainer initialized:\")\n",
        "        print(f\"  - Training samples: {len(train_loader.dataset)}\")\n",
        "        print(f\"  - Validation samples: {len(val_loader.dataset)}\")\n",
        "        print(f\"  - Epochs: {config.num_epochs}\")\n",
        "        print(f\"  - Batch size: {config.batch_size}\")\n",
        "        print(f\"  - Learning rate: {config.learning_rate}\")\n",
        "\n",
        "    def train_epoch(self, epoch: int) -> Dict[str, float]:\n",
        "        self.model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(\n",
        "            self.train_loader,\n",
        "            desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\",\n",
        "            leave=True,\n",
        "        )\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch[\"pixel_values\"].to(self.config.device)\n",
        "            input_ids = batch[\"input_ids\"].to(self.config.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.config.device)\n",
        "            labels = batch[\"labels\"].to(self.config.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            loss = self.criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Best practice: gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in self.model.parameters() if p.requires_grad], max_norm=1.0\n",
        "            )\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            self.global_step += 1\n",
        "\n",
        "            progress_bar.set_postfix(\n",
        "                {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100*correct/total:.2f}%\"}\n",
        "            )\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self) -> Dict[str, float]:\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in tqdm(self.val_loader, desc=\"Validating\", leave=False):\n",
        "            pixel_values = batch[\"pixel_values\"].to(self.config.device)\n",
        "            input_ids = batch[\"input_ids\"].to(self.config.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.config.device)\n",
        "            labels = batch[\"labels\"].to(self.config.device)\n",
        "\n",
        "            logits = self.model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            loss = self.criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
        "\n",
        "    def train(self) -> Dict[str, List[float]]:\n",
        "        history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"STARTING TRAINING\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            train_metrics = self.train_epoch(epoch)\n",
        "            val_metrics = self.validate()\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs}:\")\n",
        "            print(f\"  Train Loss: {train_metrics['loss']:.4f}, Train Acc: {100*train_metrics['accuracy']:.2f}%\")\n",
        "            print(f\"  Val Loss: {val_metrics['loss']:.4f}, Val Acc: {100*val_metrics['accuracy']:.2f}%\")\n",
        "\n",
        "            if val_metrics[\"accuracy\"] > self.best_val_accuracy:\n",
        "                self.best_val_accuracy = val_metrics[\"accuracy\"]\n",
        "                print(f\"  → New best validation accuracy: {100*self.best_val_accuracy:.2f}%\")\n",
        "\n",
        "            history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
        "            history[\"train_acc\"].append(train_metrics[\"accuracy\"])\n",
        "            history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
        "            history[\"val_acc\"].append(val_metrics[\"accuracy\"])\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TRAINING COMPLETE\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Total time: {total_time/60:.2f} minutes\")\n",
        "        print(f\"Best validation accuracy: {100*self.best_val_accuracy:.2f}%\")\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmqIpQ5EFr-1"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_model(\n",
        "    model: nn.Module, data_loader: DataLoader, config: Config, model_name: str = \"Model\"\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"Evaluate a model on a dataset.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        pixel_values = batch[\"pixel_values\"].to(config.device)\n",
        "        input_ids = batch[\"input_ids\"].to(config.device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(config.device)\n",
        "        labels = batch[\"labels\"].to(config.device)\n",
        "\n",
        "        logits = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_predictions.extend(predictions.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"  Accuracy: {100*accuracy:.2f}%\")\n",
        "    print(f\"  Correct: {correct}/{total}\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "        \"predictions\": all_predictions,\n",
        "        \"labels\": all_labels,\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_models(\n",
        "    baseline_results: Dict,\n",
        "    vpt_results: Dict,\n",
        "    baseline_params: int,\n",
        "    vpt_trainable_params: int,\n",
        "    vpt_training_time: float,\n",
        ") -> None:\n",
        "    \"\"\"Print a comparison between baseline and VPT models.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"MODEL COMPARISON: BASELINE CLIP vs CLIP + VPT\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\n ACCURACY COMPARISON:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline CLIP (Zero-shot): {100*baseline_results['accuracy']:.2f}%\")\n",
        "    print(f\"  CLIP + VPT (Trained):      {100*vpt_results['accuracy']:.2f}%\")\n",
        "\n",
        "    improvement = vpt_results[\"accuracy\"] - baseline_results[\"accuracy\"]\n",
        "    print(f\"\\n  Improvement: {100*improvement:+.2f}%\")\n",
        "\n",
        "    print(\"\\n PARAMETER COMPARISON:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline trainable params: 0 (all frozen)\")\n",
        "    print(f\"  VPT trainable params:      {vpt_trainable_params:,}\")\n",
        "    print(f\"  Trainable ratio:           {100*vpt_trainable_params/baseline_params:.4f}%\")\n",
        "\n",
        "    print(\"\\n TRAINING TIME:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline: 0 (no training)\")\n",
        "    print(f\"  VPT:      {vpt_training_time/60:.2f} minutes\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    print(\"\\n INTERPRETATION:\")\n",
        "    if improvement > 0:\n",
        "        print(f\"  → VPT improved accuracy by {100*improvement:.2f}% over zero-shot CLIP\")\n",
        "        print(f\"  → This was achieved by training only {vpt_trainable_params:,} parameters\")\n",
        "        print(f\"  → VPT is {baseline_params/vpt_trainable_params:.0f}x more parameter-efficient than full fine-tuning\")\n",
        "    else:\n",
        "        print(f\" VPT did not improve over baseline in this experiment\")\n",
        "        print(f\" Consider: adjusting prompt count, learning rate, or training longer\")\n",
        "\n",
        "\n",
        "def save_model(model: CLIPWithVPT, save_path: str, config: Config) -> None:\n",
        "    \"\"\"Save the trained VPT model (only prompt tokens, backbone is frozen).\"\"\"\n",
        "    save_dir = os.path.dirname(save_path)\n",
        "    if save_dir and not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    checkpoint = {\n",
        "        'visual_prompts': model.visual_prompts.state_dict(),\n",
        "        'config': {\n",
        "            'model_name': config.model_name,\n",
        "            'num_prompt_tokens': config.num_prompt_tokens,\n",
        "            'prompt_dim': config.prompt_dim,\n",
        "            'prompt_dropout': config.prompt_dropout,\n",
        "        }\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"\\n Model saved to: {save_path}\")\n",
        "    print(f\"   Saved prompt parameters: {model.count_trainable_parameters():,}\")\n",
        "\n",
        "\n",
        "def load_model(load_path: str, config: Optional[Config] = None) -> CLIPWithVPT:\n",
        "    \"\"\"Load a saved VPT model.\"\"\"\n",
        "    checkpoint = torch.load(load_path, map_location='cpu')\n",
        "\n",
        "    if config is None:\n",
        "        config = Config()\n",
        "        saved_config = checkpoint['config']\n",
        "        config.model_name = saved_config['model_name']\n",
        "        config.num_prompt_tokens = saved_config['num_prompt_tokens']\n",
        "        config.prompt_dim = saved_config['prompt_dim']\n",
        "        config.prompt_dropout = saved_config['prompt_dropout']\n",
        "\n",
        "    model = CLIPWithVPT(config)\n",
        "    model.visual_prompts.load_state_dict(checkpoint['visual_prompts'])\n",
        "\n",
        "    print(f\"\\n Model loaded from: {load_path}\")\n",
        "    print(f\"   Loaded prompt parameters: {model.count_trainable_parameters():,}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gntZmFnMFvKF"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(dataset: GDVCRDataset, config: Config) -> Tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"Split dataset and create train/val data loaders.\"\"\"\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(total_size * config.val_split)\n",
        "    train_size = total_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(config.seed),\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset split:\")\n",
        "    print(f\"  Training samples: {len(train_dataset)}\")\n",
        "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if config.device == \"cuda\" else False,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if config.device == \"cuda\" else False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def run_experiment(config: Config) -> None:\n",
        "    \"\"\"Run the complete VPT experiment.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VISUAL PROMPT TUNING EXPERIMENT\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Model: {config.model_name}\")\n",
        "    print(f\"  Device: {config.device}\")\n",
        "    print(f\"  Prompt tokens: {config.num_prompt_tokens}\")\n",
        "    print(f\"  Learning rate: {config.learning_rate}\")\n",
        "    print(f\"  Batch size: {config.batch_size}\")\n",
        "    print(f\"  Epochs: {config.num_epochs}\")\n",
        "\n",
        "    print(\"\\n[Step 1] Setting random seed for reproducibility...\")\n",
        "    set_seed(config.seed)\n",
        "\n",
        "    print(\"\\n[Step 2] Loading CLIP processor...\")\n",
        "    processor = CLIPProcessor.from_pretrained(config.model_name)\n",
        "\n",
        "    print(\"\\n[Step 3] Loading GD-VCR dataset...\")\n",
        "    dataset = GDVCRDataset(\n",
        "        jsonl_path=config.dataset_path,\n",
        "        processor=processor,\n",
        "        config=config,\n",
        "        image_base_path=config.image_base_path,\n",
        "    )\n",
        "\n",
        "    print(\"\\n[Step 4] Creating data loaders...\")\n",
        "    train_loader, val_loader = create_data_loaders(dataset, config)\n",
        "\n",
        "    print(\"\\n[Step 5] Evaluating baseline CLIP (zero-shot)...\")\n",
        "    baseline_model = BaselineCLIP(config)\n",
        "    baseline_results = evaluate_model(baseline_model, val_loader, config, \"Baseline CLIP\")\n",
        "    baseline_params = baseline_model.count_parameters()\n",
        "\n",
        "    del baseline_model\n",
        "    if config.device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n[Step 6] Training CLIP + VPT...\")\n",
        "    vpt_model = CLIPWithVPT(config)\n",
        "    vpt_trainable_params = vpt_model.count_trainable_parameters()\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=vpt_model, train_loader=train_loader, val_loader=val_loader, config=config\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(\"\\n[Step 7] Evaluating trained CLIP + VPT...\")\n",
        "    vpt_results = evaluate_model(vpt_model, val_loader, config, \"CLIP + VPT\")\n",
        "\n",
        "    print(\"\\n[Step 8] Comparing models...\")\n",
        "    compare_models(\n",
        "        baseline_results=baseline_results,\n",
        "        vpt_results=vpt_results,\n",
        "        baseline_params=baseline_params,\n",
        "        vpt_trainable_params=vpt_trainable_params,\n",
        "        vpt_training_time=training_time,\n",
        "    )\n",
        "\n",
        "    print(\"\\n TRAINING HISTORY SUMMARY:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Initial train acc: {100*history['train_acc'][0]:.2f}%\")\n",
        "    print(f\"  Final train acc:   {100*history['train_acc'][-1]:.2f}%\")\n",
        "    print(f\"  Initial val acc:   {100*history['val_acc'][0]:.2f}%\")\n",
        "    print(f\"  Final val acc:     {100*history['val_acc'][-1]:.2f}%\")\n",
        "\n",
        "    print(\"\\n[Step 9] Saving trained model...\")\n",
        "    save_path = \"clip_vpt_model.pt\"\n",
        "    save_model(vpt_model, save_path, config)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"EXPERIMENT COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return history, vpt_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmwOaWVKFyE1"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main entry point for the VPT experiment.\"\"\"\n",
        "    config = Config()\n",
        "\n",
        "    try:\n",
        "        history, model = run_experiment(config)\n",
        "        print(\"\\n Experiment completed successfully!\")\n",
        "        return history, model\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\n Error: Dataset file not found: {e}\")\n",
        "        print(\"Please update the dataset_path in the Config class.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error during experiment: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Quick test function to verify the pipeline works with synthetic data.\"\"\"\n",
        "    print(\"Running quick test with synthetic data...\")\n",
        "\n",
        "    config = Config()\n",
        "    config.num_epochs = 1\n",
        "    config.batch_size = 2\n",
        "\n",
        "    synthetic_data = [\n",
        "        {\n",
        "            \"img_fn\": \"test.jpg\",\n",
        "            \"objects\": [\"person\", \"dog\", \"car\"],\n",
        "            \"question\": [\"What\", \"is\", [0], \"doing\", \"?\"],\n",
        "            \"answer_choices\": [\n",
        "                [\"Walking\", \"the\", [1], \".\"],\n",
        "                [\"Driving\", \"the\", [2], \".\"],\n",
        "                [\"Sleeping\", \".\"],\n",
        "                [\"Running\", \".\"],\n",
        "            ],\n",
        "            \"answer_label\": 0,\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    with open(\"test_data.jsonl\", \"w\") as f:\n",
        "        for item in synthetic_data:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "    test_image = Image.new(\"RGB\", (224, 224), color=\"white\")\n",
        "    test_image.save(\"test.jpg\")\n",
        "\n",
        "    print(\"\\nTesting preprocessing...\")\n",
        "    for record in synthetic_data:\n",
        "        processed = preprocess_sample(record)\n",
        "        print(f\"  Question: {processed['question']}\")\n",
        "        print(f\"  Answers: {processed['answer_choices']}\")\n",
        "\n",
        "    print(\"\\n Quick test passed! The pipeline is working.\")\n",
        "\n",
        "    os.remove(\"test_data.jsonl\")\n",
        "    os.remove(\"test.jpg\")\n",
        "\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies for Google Colab.\"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = [\"torch\", \"transformers\", \"Pillow\", \"tqdm\"]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "    print(\"\\n All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f2bf059dd96f497a9ddaa4fb9dacda19",
            "7633bdb0b8644a26bebdde36f456d40b",
            "8faad3d3c86548228345c2aee5d20a90",
            "a66d94207ab24ca29a5ae034c18c9462",
            "f1cd41af7ab9449291fd2a5be1fc29f9",
            "ed4d85b50771425c84a5e6454ed2b17b",
            "d7040bd4bfd9444eb0248a8974c2f8c6",
            "d73ba85a5bba475caf5ef4856b018230",
            "fbf7c5d23bcc4e47bc658915edd11b58",
            "c6596bd98447472e914b79206e9add9d",
            "962a7d61f9244b249f6637535533df65",
            "90e58a053072490298e037ca7664a71b",
            "86b0cce23ed647b7bfd771cad196f379",
            "6da348f1a91f4e73b6a42fc3f4778c86",
            "542c7b8bccca40ee867405ec1f42f437",
            "91b9ce7d9d3747b885733d5bc9e2328b",
            "b2c27d2addb747eea4c1d3bd52d222ec",
            "cae7e46c7d0742e692334ebc1d4d2176",
            "85d562d787ff4d17b1a27298388838e9",
            "374689e014aa47a2bae5c3c3f3623fbe",
            "bcdaaffc59de4284be929753344bee77",
            "68eac43c65834ab993e6e7c6b4cb9f76"
          ]
        },
        "id": "EHQml1PDFz-S",
        "outputId": "bc911b06-ff1b-4de9-8ddc-1fb8882fc714"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "374689e014aa47a2bae5c3c3f3623fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "542c7b8bccca40ee867405ec1f42f437": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bcdaaffc59de4284be929753344bee77",
            "placeholder": "​",
            "style": "IPY_MODEL_68eac43c65834ab993e6e7c6b4cb9f76",
            "value": " 398/398 [00:00&lt;00:00, 819.94it/s, Materializing param=visual_projection.weight]"
          }
        },
        "68eac43c65834ab993e6e7c6b4cb9f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6da348f1a91f4e73b6a42fc3f4778c86": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85d562d787ff4d17b1a27298388838e9",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_374689e014aa47a2bae5c3c3f3623fbe",
            "value": 398
          }
        },
        "7633bdb0b8644a26bebdde36f456d40b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed4d85b50771425c84a5e6454ed2b17b",
            "placeholder": "​",
            "style": "IPY_MODEL_d7040bd4bfd9444eb0248a8974c2f8c6",
            "value": "Loading weights: 100%"
          }
        },
        "85d562d787ff4d17b1a27298388838e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86b0cce23ed647b7bfd771cad196f379": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2c27d2addb747eea4c1d3bd52d222ec",
            "placeholder": "​",
            "style": "IPY_MODEL_cae7e46c7d0742e692334ebc1d4d2176",
            "value": "Loading weights: 100%"
          }
        },
        "8faad3d3c86548228345c2aee5d20a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d73ba85a5bba475caf5ef4856b018230",
            "max": 398,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fbf7c5d23bcc4e47bc658915edd11b58",
            "value": 398
          }
        },
        "90e58a053072490298e037ca7664a71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_86b0cce23ed647b7bfd771cad196f379",
              "IPY_MODEL_6da348f1a91f4e73b6a42fc3f4778c86",
              "IPY_MODEL_542c7b8bccca40ee867405ec1f42f437"
            ],
            "layout": "IPY_MODEL_91b9ce7d9d3747b885733d5bc9e2328b"
          }
        },
        "91b9ce7d9d3747b885733d5bc9e2328b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "962a7d61f9244b249f6637535533df65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a66d94207ab24ca29a5ae034c18c9462": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6596bd98447472e914b79206e9add9d",
            "placeholder": "​",
            "style": "IPY_MODEL_962a7d61f9244b249f6637535533df65",
            "value": " 398/398 [00:00&lt;00:00, 805.70it/s, Materializing param=visual_projection.weight]"
          }
        },
        "b2c27d2addb747eea4c1d3bd52d222ec": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcdaaffc59de4284be929753344bee77": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6596bd98447472e914b79206e9add9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cae7e46c7d0742e692334ebc1d4d2176": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7040bd4bfd9444eb0248a8974c2f8c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d73ba85a5bba475caf5ef4856b018230": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed4d85b50771425c84a5e6454ed2b17b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1cd41af7ab9449291fd2a5be1fc29f9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2bf059dd96f497a9ddaa4fb9dacda19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7633bdb0b8644a26bebdde36f456d40b",
              "IPY_MODEL_8faad3d3c86548228345c2aee5d20a90",
              "IPY_MODEL_a66d94207ab24ca29a5ae034c18c9462"
            ],
            "layout": "IPY_MODEL_f1cd41af7ab9449291fd2a5be1fc29f9"
          }
        },
        "fbf7c5d23bcc4e47bc658915edd11b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
