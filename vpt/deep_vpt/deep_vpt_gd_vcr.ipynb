{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAxNwjhsKm1c"
      },
      "source": [
        "\n",
        "\n",
        "# Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhIc74wDqGMO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel, CLIPTextModel\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BK_3VtgMqIQV",
        "outputId": "898c6a58-b82d-466d-8cd9-897f92784fb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mxn9JBxyKqw7"
      },
      "source": [
        "# CONFIGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBwB5nReqIWh"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Central configuration class for all hyperparameters and paths.\"\"\"\n",
        "\n",
        "    # Model\n",
        "    model_name: str = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "    # Dataset paths\n",
        "    dataset_path: str = \"/content/drive/MyDrive/GD-VCR-Dataset/images-vcr-drive.jsonl\"\n",
        "    image_base_path: str = \"/content/drive/MyDrive/GD-VCR-Dataset/MC-VCR_sample/\"\n",
        "\n",
        "    # VPT Hyperparameters\n",
        "    num_prompt_tokens: int = 50\n",
        "    prompt_dim: int = 1024\n",
        "    prompt_dropout: float = 0.1\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    batch_size: int = 16\n",
        "    learning_rate: float = 1e-3\n",
        "    num_epochs: int = 10\n",
        "    warmup_steps: int = 100\n",
        "    weight_decay: float = 0.01\n",
        "\n",
        "    # System\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed: int = 42\n",
        "    num_workers: int = 2\n",
        "\n",
        "    # Validation split\n",
        "    val_split: float = 0.3\n",
        "\n",
        "    # Maximum sequence length for text\n",
        "    max_text_length: int = 77\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    \"\"\"Set random seeds for reproducibility across all libraries.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # For complete reproducibility (may slow down training)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCrxmKoIcxCf"
      },
      "source": [
        "# Dataset Parsing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sT6KqFJ7qIaS"
      },
      "outputs": [],
      "source": [
        "def load_jsonl(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Load a JSONL file and return a list of dictionaries.\"\"\"\n",
        "    data = []\n",
        "\n",
        "    print(f\"Loading dataset from: {file_path}\")\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    data.append(record)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Warning: Could not parse line {line_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    print(f\"Loaded {len(data)} records from {file_path}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def validate_record(record: Dict) -> bool:\n",
        "    \"\"\"Validate that a GD-VCR record has all required fields.\"\"\"\n",
        "    required_fields = [\n",
        "        \"img_fn\",\n",
        "        \"objects\",\n",
        "        \"question\",\n",
        "        \"answer_choices\",\n",
        "        \"answer_label\",\n",
        "    ]\n",
        "\n",
        "    for field in required_fields:\n",
        "        if field not in record:\n",
        "            return False\n",
        "\n",
        "    if record[\"answer_label\"] < 0 or record[\"answer_label\"] >= len(\n",
        "        record[\"answer_choices\"]\n",
        "    ):\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUrO1WI7csZH"
      },
      "source": [
        "# Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ana8Xf7UqIeM"
      },
      "outputs": [],
      "source": [
        "def resolve_object_reference(token: Any, objects: List[str]) -> str:\n",
        "    \"\"\"Resolve an object reference token to its actual object name.\"\"\"\n",
        "    if isinstance(token, list) and len(token) == 1 and isinstance(token[0], int):\n",
        "        index = token[0]\n",
        "        if 0 <= index < len(objects):\n",
        "            return objects[index]\n",
        "        else:\n",
        "            return f\"object{index}\"\n",
        "\n",
        "    if isinstance(token, int):\n",
        "        if 0 <= token < len(objects):\n",
        "            return objects[token]\n",
        "        else:\n",
        "            return f\"object{token}\"\n",
        "\n",
        "    return str(token)\n",
        "\n",
        "\n",
        "def tokens_to_sentence(tokens: List[Any], objects: List[str]) -> str:\n",
        "    \"\"\"Convert a list of tokens (with object references) into a natural sentence.\"\"\"\n",
        "    resolved_tokens = [resolve_object_reference(token, objects) for token in tokens]\n",
        "\n",
        "    sentence = \" \".join(resolved_tokens)\n",
        "\n",
        "    punctuation = [\".\", \",\", \"!\", \"?\", \"'\", '\"', \":\", \";\"]\n",
        "    for punct in punctuation:\n",
        "        sentence = sentence.replace(f\" {punct}\", punct)\n",
        "\n",
        "    sentence = sentence.replace(\" ' s\", \"'s\")\n",
        "    sentence = sentence.replace(\" 's\", \"'s\")\n",
        "\n",
        "    sentence = \" \".join(sentence.split())\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def preprocess_sample(record: Dict) -> Dict:\n",
        "    \"\"\"Preprocess a single GD-VCR record into a format suitable for CLIP.\"\"\"\n",
        "    objects = record[\"objects\"]\n",
        "\n",
        "    question = tokens_to_sentence(record[\"question\"], objects)\n",
        "\n",
        "    answer_choices = []\n",
        "    for choice_tokens in record[\"answer_choices\"]:\n",
        "        choice_sentence = tokens_to_sentence(choice_tokens, objects)\n",
        "        answer_choices.append(choice_sentence)\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer_choices\": answer_choices,\n",
        "        \"answer_label\": record[\"answer_label\"],\n",
        "        \"img_fn\": record[\"img_fn\"],\n",
        "        \"objects\": objects,\n",
        "    }\n",
        "\n",
        "\n",
        "def create_qa_text(question: str, answer: str) -> str:\n",
        "    \"\"\"Combine question and answer into a single text for CLIP encoding.\"\"\"\n",
        "    return f\"Question: {question} Answer: {answer}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bFlaD1PcnZU"
      },
      "source": [
        "# Dataset Class\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgdRtmuHqIh1"
      },
      "outputs": [],
      "source": [
        "class GDVCRDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for GD-VCR visual commonsense reasoning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        jsonl_path: str,\n",
        "        processor: CLIPProcessor,\n",
        "        config: Config,\n",
        "        image_base_path: Optional[str] = None,\n",
        "    ):\n",
        "        \"\"\"Initialize the GD-VCR dataset.\"\"\"\n",
        "        self.processor = processor\n",
        "        self.config = config\n",
        "        self.image_base_path = image_base_path or config.image_base_path\n",
        "\n",
        "        raw_data = load_jsonl(jsonl_path)\n",
        "\n",
        "        self.data = []\n",
        "        for record in raw_data:\n",
        "            if validate_record(record):\n",
        "                preprocessed = preprocess_sample(record)\n",
        "                self.data.append(preprocessed)\n",
        "            else:\n",
        "                print(f\"Warning: Skipping invalid record\")\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.data)} valid samples\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def load_image(self, img_fn: str) -> Image.Image:\n",
        "        \"\"\"Load an image from disk.\"\"\"\n",
        "        img_path = os.path.join(self.image_base_path, img_fn)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            return image\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image not found: {img_path}\")\n",
        "            return Image.new(\"RGB\", (224, 224), color=\"gray\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error loading {img_path}: {e}\")\n",
        "            return Image.new(\"RGB\", (224, 224), color=\"gray\")\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Get a single sample from the dataset.\"\"\"\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        image = self.load_image(sample[\"img_fn\"])\n",
        "\n",
        "        image_inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = image_inputs[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        question = sample[\"question\"]\n",
        "        answer_choices = sample[\"answer_choices\"]\n",
        "\n",
        "        qa_texts = []\n",
        "        for answer in answer_choices:\n",
        "            qa_text = create_qa_text(question, answer)\n",
        "            qa_texts.append(qa_text)\n",
        "\n",
        "        text_inputs = self.processor(\n",
        "            text=qa_texts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.config.max_text_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"input_ids\": text_inputs[\"input_ids\"],\n",
        "            \"attention_mask\": text_inputs[\"attention_mask\"],\n",
        "            \"label\": torch.tensor(sample[\"answer_label\"], dtype=torch.long),\n",
        "            \"num_choices\": len(answer_choices),\n",
        "            \"img_fn\": sample[\"img_fn\"],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Custom collate function for batching GD-VCR samples.\"\"\"\n",
        "    batch_size = len(batch)\n",
        "\n",
        "    max_choices = max(sample[\"num_choices\"] for sample in batch)\n",
        "    max_length = batch[0][\"input_ids\"].shape[1]\n",
        "\n",
        "    pixel_values = torch.stack([sample[\"pixel_values\"] for sample in batch])\n",
        "    labels = torch.stack([sample[\"label\"] for sample in batch])\n",
        "    num_choices = torch.tensor([sample[\"num_choices\"] for sample in batch])\n",
        "\n",
        "    input_ids = torch.zeros(batch_size, max_choices, max_length, dtype=torch.long)\n",
        "    attention_mask = torch.zeros(batch_size, max_choices, max_length, dtype=torch.long)\n",
        "\n",
        "    for i, sample in enumerate(batch):\n",
        "        n_choices = sample[\"num_choices\"]\n",
        "        input_ids[i, :n_choices] = sample[\"input_ids\"]\n",
        "        attention_mask[i, :n_choices] = sample[\"attention_mask\"]\n",
        "\n",
        "    img_fns = [sample[\"img_fn\"] for sample in batch]\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"num_choices\": num_choices,\n",
        "        \"img_fns\": img_fns,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRxwbIn3cjSM"
      },
      "source": [
        "# Baseline CLIP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCCHkeG1qIlL"
      },
      "outputs": [],
      "source": [
        "class BaselineCLIP(nn.Module):\n",
        "    \"\"\"Baseline CLIP model for visual commonsense reasoning (zero-shot).\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        \"\"\"Initialize the baseline CLIP model.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"Loading CLIP model: {config.model_name}\")\n",
        "\n",
        "        self.clip = CLIPModel.from_pretrained(config.model_name)\n",
        "\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.to(config.device)\n",
        "\n",
        "        print(\n",
        "            f\"Baseline CLIP loaded with {self.count_parameters():,} parameters (all frozen)\"\n",
        "        )\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        \"\"\"Count total number of parameters.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def count_trainable_parameters(self) -> int:\n",
        "        \"\"\"Count trainable parameters (should be 0 for baseline).\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def encode_image(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Encode images using CLIP's vision encoder.\"\"\"\n",
        "        image_outputs = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "\n",
        "        if hasattr(image_outputs, \"pooler_output\"):\n",
        "            image_features = image_outputs.pooler_output\n",
        "        else:\n",
        "            image_features = image_outputs\n",
        "\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "\n",
        "        return image_features\n",
        "\n",
        "    def encode_text(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Encode text using CLIP's text encoder.\"\"\"\n",
        "        batch_size, num_choices, seq_len = input_ids.shape\n",
        "\n",
        "        input_ids_flat = input_ids.view(-1, seq_len)\n",
        "        attention_mask_flat = attention_mask.view(-1, seq_len)\n",
        "\n",
        "        text_outputs = self.clip.get_text_features(\n",
        "            input_ids=input_ids_flat, attention_mask=attention_mask_flat\n",
        "        )\n",
        "\n",
        "        if hasattr(text_outputs, \"pooler_output\"):\n",
        "            text_features = text_outputs.pooler_output\n",
        "        else:\n",
        "            text_features = text_outputs\n",
        "\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "        text_features = text_features.view(batch_size, num_choices, -1)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.Tensor,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        **kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Forward pass computing similarity scores for each answer choice.\"\"\"\n",
        "        image_features = self.encode_image(pixel_values)\n",
        "\n",
        "        text_features = self.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "\n",
        "        logits = torch.einsum(\n",
        "            \"bcd,bcd->bc\", image_features.expand_as(text_features), text_features\n",
        "        )\n",
        "\n",
        "        logit_scale = self.clip.logit_scale.exp()\n",
        "        logits = logits * logit_scale\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QdLxLJ-chDm"
      },
      "source": [
        "# VPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wu_lVURuqIn5"
      },
      "outputs": [],
      "source": [
        "class VisualPromptTokens(nn.Module):\n",
        "    \"\"\"Learnable visual prompt tokens for Deep VPT.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, num_prompts: int, hidden_dim: int, num_layers: int, dropout: float = 0.1\n",
        "    ):\n",
        "        \"\"\"Initialize the visual prompt tokens for Deep VPT.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_prompts = num_prompts\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Learnable prompts for ALL layers: [num_layers, 1, num_prompts, hidden_dim]\n",
        "        self.prompts = nn.Parameter(torch.zeros(num_layers, 1, num_prompts, hidden_dim))\n",
        "\n",
        "        # Xavier uniform initialization for proper variance\n",
        "        nn.init.xavier_uniform_(self.prompts.view(-1, hidden_dim))\n",
        "        self.prompts.data = self.prompts.data.view(\n",
        "            num_layers, 1, num_prompts, hidden_dim\n",
        "        )\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        total_params = num_layers * num_prompts * hidden_dim\n",
        "        print(\n",
        "            f\"Created Deep VPT with {num_prompts} prompts per layer x {num_layers} layers\"\n",
        "        )\n",
        "        print(f\"Total prompt parameters: {total_params:,}\")\n",
        "\n",
        "    def forward(self, batch_size: int, layer_idx: int) -> torch.Tensor:\n",
        "        \"\"\"Get prompt tokens for a specific layer expanded to batch size.\"\"\"\n",
        "        prompts = self.prompts[layer_idx].expand(batch_size, -1, -1)\n",
        "\n",
        "        prompts = self.dropout(prompts)\n",
        "\n",
        "        return prompts\n",
        "\n",
        "\n",
        "class CLIPWithVPT(nn.Module):\n",
        "    \"\"\"CLIP model with Deep Visual Prompt Tuning.\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        \"\"\"Initialize CLIP with VPT.\"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        print(f\"Loading CLIP model for VPT: {config.model_name}\")\n",
        "        self.clip = CLIPModel.from_pretrained(config.model_name)\n",
        "\n",
        "        # Freeze ALL CLIP parameters - only prompts will be trained\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.num_layers = self.clip.vision_model.encoder.config.num_hidden_layers\n",
        "        print(f\"Vision encoder has {self.num_layers} transformer layers\")\n",
        "\n",
        "        self.visual_prompts = VisualPromptTokens(\n",
        "            num_prompts=config.num_prompt_tokens,\n",
        "            hidden_dim=config.prompt_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            dropout=config.prompt_dropout,\n",
        "        )\n",
        "\n",
        "        self.to(config.device)\n",
        "\n",
        "        total_params = self.count_parameters()\n",
        "        trainable_params = self.count_trainable_parameters()\n",
        "        print(f\"Total parameters: {total_params:,}\")\n",
        "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"Trainable ratio: {100*trainable_params/total_params:.4f}%\")\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        \"\"\"Count total number of parameters.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def count_trainable_parameters(self) -> int:\n",
        "        \"\"\"Count trainable parameters.\"\"\"\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def get_visual_embeddings_with_prompts(\n",
        "        self, pixel_values: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Get visual embeddings with Deep VPT - prompts injected at EVERY layer.\"\"\"\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        num_prompts = self.visual_prompts.num_prompts\n",
        "\n",
        "        vision_model = self.clip.vision_model\n",
        "\n",
        "        embeddings = vision_model.embeddings(pixel_values)\n",
        "\n",
        "        cls_token = embeddings[:, :1, :]\n",
        "        patch_embeddings = embeddings[:, 1:, :]\n",
        "\n",
        "        hidden_states = vision_model.pre_layrnorm(embeddings)\n",
        "        cls_token = hidden_states[:, :1, :]\n",
        "        patch_embeddings = hidden_states[:, 1:, :]\n",
        "\n",
        "        encoder = vision_model.encoder\n",
        "\n",
        "        for layer_idx, layer in enumerate(encoder.layers):\n",
        "            layer_prompts = self.visual_prompts(batch_size, layer_idx)\n",
        "\n",
        "            hidden_states = torch.cat(\n",
        "                [cls_token, layer_prompts, patch_embeddings],\n",
        "                dim=1,\n",
        "            )\n",
        "\n",
        "            layer_outputs = layer(\n",
        "                hidden_states,\n",
        "                attention_mask=None,\n",
        "                causal_attention_mask=None,\n",
        "                output_attentions=False,\n",
        "            )\n",
        "\n",
        "            if isinstance(layer_outputs, tuple):\n",
        "                hidden_states = layer_outputs[0]\n",
        "            else:\n",
        "                hidden_states = layer_outputs\n",
        "\n",
        "            cls_token = hidden_states[:, :1, :]\n",
        "            patch_embeddings = hidden_states[:, 1 + num_prompts :, :]\n",
        "\n",
        "        pooled_output = hidden_states[:, 0, :]\n",
        "\n",
        "        pooled_output = vision_model.post_layernorm(pooled_output)\n",
        "\n",
        "        image_features = self.clip.visual_projection(pooled_output)\n",
        "\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "\n",
        "        return image_features\n",
        "\n",
        "    def encode_text(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Encode text using CLIP's frozen text encoder.\"\"\"\n",
        "        batch_size, num_choices, seq_len = input_ids.shape\n",
        "\n",
        "        input_ids_flat = input_ids.view(-1, seq_len)\n",
        "        attention_mask_flat = attention_mask.view(-1, seq_len)\n",
        "\n",
        "        text_output = self.clip.get_text_features(\n",
        "            input_ids=input_ids_flat, attention_mask=attention_mask_flat\n",
        "        )\n",
        "\n",
        "        if hasattr(text_output, \"pooler_output\"):\n",
        "            text_features = text_output.pooler_output\n",
        "        elif hasattr(text_output, \"last_hidden_state\"):\n",
        "            text_features = text_output.last_hidden_state[:, 0, :]\n",
        "        elif isinstance(text_output, torch.Tensor):\n",
        "            text_features = text_output\n",
        "        else:\n",
        "            text_features = (\n",
        "                text_output[0] if isinstance(text_output, tuple) else text_output\n",
        "            )\n",
        "\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "\n",
        "        text_features = text_features.view(batch_size, num_choices, -1)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.Tensor,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        **kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Forward pass with visual prompts.\"\"\"\n",
        "        image_features = self.get_visual_embeddings_with_prompts(pixel_values)\n",
        "\n",
        "        text_features = self.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        logits = torch.einsum(\n",
        "            \"bcd,bcd->bc\", image_features.expand_as(text_features), text_features\n",
        "        )\n",
        "\n",
        "        logit_scale = self.clip.logit_scale.exp()\n",
        "        logits = logits * logit_scale\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DA7Pad0DccqC"
      },
      "source": [
        "# Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YmxvceGtqIq6"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Training manager for VPT models.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        config: Config,\n",
        "    ):\n",
        "        \"\"\"Initialize the trainer.\"\"\"\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "\n",
        "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "        self.optimizer = AdamW(\n",
        "            trainable_params, lr=config.learning_rate, weight_decay=config.weight_decay\n",
        "        )\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        total_steps = len(train_loader) * config.num_epochs\n",
        "\n",
        "        def lr_lambda(step):\n",
        "            if step < config.warmup_steps:\n",
        "                return float(step) / float(max(1, config.warmup_steps))\n",
        "            return 1.0\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.best_val_accuracy = 0.0\n",
        "\n",
        "        print(f\"Trainer initialized:\")\n",
        "        print(f\"  - Training samples: {len(train_loader.dataset)}\")\n",
        "        print(f\"  - Validation samples: {len(val_loader.dataset)}\")\n",
        "        print(f\"  - Epochs: {config.num_epochs}\")\n",
        "        print(f\"  - Batch size: {config.batch_size}\")\n",
        "        print(f\"  - Learning rate: {config.learning_rate}\")\n",
        "\n",
        "    def train_epoch(self, epoch: int) -> Dict[str, float]:\n",
        "        \"\"\"Train for one epoch.\"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(\n",
        "            self.train_loader,\n",
        "            desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\",\n",
        "            leave=True,\n",
        "        )\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch[\"pixel_values\"].to(self.config.device)\n",
        "            input_ids = batch[\"input_ids\"].to(self.config.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.config.device)\n",
        "            labels = batch[\"labels\"].to(self.config.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            loss = self.criterion(logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in self.model.parameters() if p.requires_grad], max_norm=1.0\n",
        "            )\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            self.global_step += 1\n",
        "\n",
        "            progress_bar.set_postfix(\n",
        "                {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100*correct/total:.2f}%\"}\n",
        "            )\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self) -> Dict[str, float]:\n",
        "        \"\"\"Validate the model on the validation set.\"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in tqdm(self.val_loader, desc=\"Validating\", leave=False):\n",
        "            pixel_values = batch[\"pixel_values\"].to(self.config.device)\n",
        "            input_ids = batch[\"input_ids\"].to(self.config.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.config.device)\n",
        "            labels = batch[\"labels\"].to(self.config.device)\n",
        "\n",
        "            logits = self.model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            loss = self.criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
        "\n",
        "    def train(self) -> Dict[str, List[float]]:\n",
        "        \"\"\"Full training loop.\"\"\"\n",
        "        history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"STARTING TRAINING\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            train_metrics = self.train_epoch(epoch)\n",
        "\n",
        "            val_metrics = self.validate()\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs}:\")\n",
        "            print(\n",
        "                f\"  Train Loss: {train_metrics['loss']:.4f}, Train Acc: {100*train_metrics['accuracy']:.2f}%\"\n",
        "            )\n",
        "            print(\n",
        "                f\"  Val Loss: {val_metrics['loss']:.4f}, Val Acc: {100*val_metrics['accuracy']:.2f}%\"\n",
        "            )\n",
        "\n",
        "            if val_metrics[\"accuracy\"] > self.best_val_accuracy:\n",
        "                self.best_val_accuracy = val_metrics[\"accuracy\"]\n",
        "                print(\n",
        "                    f\"  → New best validation accuracy: {100*self.best_val_accuracy:.2f}%\"\n",
        "                )\n",
        "\n",
        "            history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
        "            history[\"train_acc\"].append(train_metrics[\"accuracy\"])\n",
        "            history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
        "            history[\"val_acc\"].append(val_metrics[\"accuracy\"])\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TRAINING COMPLETE\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Total time: {total_time/60:.2f} minutes\")\n",
        "        print(f\"Best validation accuracy: {100*self.best_val_accuracy:.2f}%\")\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClEpunFocY91"
      },
      "source": [
        "# Eval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_IUawiMGqIs2"
      },
      "outputs": [],
      "source": [
        "REGION_NAMES = [\"East Asia\", \"South Asia\", \"Africa\", \"West\"]\n",
        "\n",
        "\n",
        "def get_region_from_filename(img_fn: str) -> Optional[str]:\n",
        "    \"\"\"Infer cultural region from image filename prefix.\"\"\"\n",
        "    import re\n",
        "\n",
        "    filename = os.path.basename(img_fn)\n",
        "\n",
        "    if filename.startswith(\"jpn_\") or filename.startswith(\"kor_\"):\n",
        "        return \"East Asia\"\n",
        "\n",
        "    if filename.startswith(\"af_\"):\n",
        "        return \"Africa\"\n",
        "\n",
        "    if filename.startswith(\"sa_\"):\n",
        "        return \"South Asia\"\n",
        "\n",
        "    if filename.startswith(\"west_\"):\n",
        "        return \"West\"\n",
        "\n",
        "    if re.match(r\"^\\d+\\.jpe?g$\", filename, re.IGNORECASE):\n",
        "        return \"East Asia\"\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(\n",
        "    model: nn.Module, data_loader: DataLoader, config: Config, model_name: str = \"Model\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate a model on a dataset with per-region accuracy tracking.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    region_metrics = {region: {\"correct\": 0, \"total\": 0} for region in REGION_NAMES}\n",
        "\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        pixel_values = batch[\"pixel_values\"].to(config.device)\n",
        "        input_ids = batch[\"input_ids\"].to(config.device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(config.device)\n",
        "        labels = batch[\"labels\"].to(config.device)\n",
        "        img_fns = batch[\"img_fns\"]\n",
        "\n",
        "        logits = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_predictions.extend(predictions.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "        predictions_list = predictions.cpu().tolist()\n",
        "        labels_list = labels.cpu().tolist()\n",
        "\n",
        "        for i, img_fn in enumerate(img_fns):\n",
        "            region = get_region_from_filename(img_fn)\n",
        "            if region is not None:\n",
        "                region_metrics[region][\"total\"] += 1\n",
        "                if predictions_list[i] == labels_list[i]:\n",
        "                    region_metrics[region][\"correct\"] += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    region_accuracies = {}\n",
        "    for region in REGION_NAMES:\n",
        "        if region_metrics[region][\"total\"] > 0:\n",
        "            region_accuracies[region] = {\n",
        "                \"accuracy\": region_metrics[region][\"correct\"]\n",
        "                / region_metrics[region][\"total\"],\n",
        "                \"correct\": region_metrics[region][\"correct\"],\n",
        "                \"total\": region_metrics[region][\"total\"],\n",
        "            }\n",
        "        else:\n",
        "            region_accuracies[region] = {\n",
        "                \"accuracy\": 0.0,\n",
        "                \"correct\": 0,\n",
        "                \"total\": 0,\n",
        "            }\n",
        "\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"  Overall Accuracy: {100*accuracy:.2f}%\")\n",
        "    print(f\"  Correct: {correct}/{total}\")\n",
        "    print(f\"\\n  Accuracy by Region:\")\n",
        "    for region in REGION_NAMES:\n",
        "        r = region_accuracies[region]\n",
        "        if r[\"total\"] > 0:\n",
        "            print(\n",
        "                f\"  - {region}: {100*r['accuracy']:.2f}% ({r['correct']}/{r['total']})\"\n",
        "            )\n",
        "        else:\n",
        "            print(f\"  - {region}: N/A (0 samples)\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "        \"predictions\": all_predictions,\n",
        "        \"labels\": all_labels,\n",
        "        \"region_accuracies\": region_accuracies,\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_models(\n",
        "    baseline_results: Dict,\n",
        "    vpt_results: Dict,\n",
        "    baseline_params: int,\n",
        "    vpt_trainable_params: int,\n",
        "    vpt_training_time: float,\n",
        ") -> None:\n",
        "    \"\"\"Print a comparison between baseline and VPT models.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"MODEL COMPARISON: BASELINE CLIP vs CLIP + VPT\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\n OVERALL ACCURACY COMPARISON:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline CLIP (Zero-shot): {100*baseline_results['accuracy']:.2f}%\")\n",
        "    print(f\"  CLIP + VPT (Trained):      {100*vpt_results['accuracy']:.2f}%\")\n",
        "\n",
        "    improvement = vpt_results[\"accuracy\"] - baseline_results[\"accuracy\"]\n",
        "    print(f\"\\n  Improvement: {100*improvement:+.2f}%\")\n",
        "\n",
        "    print(\"\\n ACCURACY BY REGION:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"  {'Region':<15} {'Baseline CLIP':<25} {'CLIP + VPT':<25} {'Δ'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    baseline_regions = baseline_results.get(\"region_accuracies\", {})\n",
        "    vpt_regions = vpt_results.get(\"region_accuracies\", {})\n",
        "\n",
        "    for region in REGION_NAMES:\n",
        "        b_acc = baseline_regions.get(region, {}).get(\"accuracy\", 0.0)\n",
        "        b_correct = baseline_regions.get(region, {}).get(\"correct\", 0)\n",
        "        b_total = baseline_regions.get(region, {}).get(\"total\", 0)\n",
        "\n",
        "        v_acc = vpt_regions.get(region, {}).get(\"accuracy\", 0.0)\n",
        "        v_correct = vpt_regions.get(region, {}).get(\"correct\", 0)\n",
        "        v_total = vpt_regions.get(region, {}).get(\"total\", 0)\n",
        "\n",
        "        if b_total > 0 or v_total > 0:\n",
        "            b_str = (\n",
        "                f\"{100*b_acc:.2f}% ({b_correct}/{b_total})\" if b_total > 0 else \"N/A\"\n",
        "            )\n",
        "            v_str = (\n",
        "                f\"{100*v_acc:.2f}% ({v_correct}/{v_total})\" if v_total > 0 else \"N/A\"\n",
        "            )\n",
        "            delta = v_acc - b_acc if (b_total > 0 and v_total > 0) else 0.0\n",
        "            delta_str = f\"{100*delta:+.2f}%\" if (b_total > 0 and v_total > 0) else \"N/A\"\n",
        "            print(f\"  {region:<15} {b_str:<25} {v_str:<25} {delta_str}\")\n",
        "        else:\n",
        "            print(f\"  {region:<15} {'N/A':<25} {'N/A':<25} {'N/A'}\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    print(\"\\n PARAMETER COMPARISON:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline trainable params: 0 (all frozen)\")\n",
        "    print(f\"  VPT trainable params:      {vpt_trainable_params:,}\")\n",
        "    print(\n",
        "        f\"  Trainable ratio:           {100*vpt_trainable_params/baseline_params:.4f}%\"\n",
        "    )\n",
        "\n",
        "    print(\"\\n TRAINING TIME:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline: 0 (no training)\")\n",
        "    print(f\"  VPT:      {vpt_training_time/60:.2f} minutes\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    print(\"\\n INTERPRETATION:\")\n",
        "    if improvement > 0:\n",
        "        print(\n",
        "            f\"  → VPT improved accuracy by {100*improvement:.2f}% over zero-shot CLIP\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  → This was achieved by training only {vpt_trainable_params:,} parameters\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  → VPT is {baseline_params/vpt_trainable_params:.0f}x more parameter-efficient than full fine-tuning\"\n",
        "        )\n",
        "    else:\n",
        "        print(f\"  → VPT did not improve over baseline in this experiment\")\n",
        "        print(\n",
        "            f\"  → This might be due to: limited data, hyperparameters, or task nature\"\n",
        "        )\n",
        "        print(\n",
        "            f\"  → Consider: adjusting prompt count, learning rate, or training longer\"\n",
        "        )\n",
        "\n",
        "\n",
        "def save_model(model: CLIPWithVPT, save_path: str, config: Config) -> None:\n",
        "    \"\"\"Save the trained VPT model (only prompt tokens since CLIP is frozen).\"\"\"\n",
        "    save_dir = os.path.dirname(save_path)\n",
        "    if save_dir and not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    checkpoint = {\n",
        "        \"visual_prompts\": model.visual_prompts.state_dict(),\n",
        "        \"config\": {\n",
        "            \"model_name\": config.model_name,\n",
        "            \"num_prompt_tokens\": config.num_prompt_tokens,\n",
        "            \"prompt_dim\": config.prompt_dim,\n",
        "            \"prompt_dropout\": config.prompt_dropout,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"\\n Model saved to: {save_path}\")\n",
        "    print(f\"   Saved prompt parameters: {model.count_trainable_parameters():,}\")\n",
        "\n",
        "\n",
        "def load_model(load_path: str, config: Optional[Config] = None) -> CLIPWithVPT:\n",
        "    \"\"\"Load a saved VPT model.\"\"\"\n",
        "    checkpoint = torch.load(load_path, map_location=\"cpu\")\n",
        "\n",
        "    if config is None:\n",
        "        config = Config()\n",
        "        saved_config = checkpoint[\"config\"]\n",
        "        config.model_name = saved_config[\"model_name\"]\n",
        "        config.num_prompt_tokens = saved_config[\"num_prompt_tokens\"]\n",
        "        config.prompt_dim = saved_config[\"prompt_dim\"]\n",
        "        config.prompt_dropout = saved_config[\"prompt_dropout\"]\n",
        "\n",
        "    model = CLIPWithVPT(config)\n",
        "\n",
        "    model.visual_prompts.load_state_dict(checkpoint[\"visual_prompts\"])\n",
        "\n",
        "    print(f\"\\n Model loaded from: {load_path}\")\n",
        "    print(f\"   Loaded prompt parameters: {model.count_trainable_parameters():,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(\n",
        "    model: CLIPWithVPT,\n",
        "    image_path: str,\n",
        "    question: str,\n",
        "    answer_choices: List[str],\n",
        "    processor: Optional[CLIPProcessor] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Make a prediction using the trained VPT model.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if processor is None:\n",
        "        processor = CLIPProcessor.from_pretrained(model.config.model_name)\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    pixel_values = image_inputs[\"pixel_values\"].to(model.config.device)\n",
        "\n",
        "    qa_texts = [f\"Question: {question} Answer: {ans}\" for ans in answer_choices]\n",
        "\n",
        "    text_inputs = processor(\n",
        "        text=qa_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=77,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    input_ids = text_inputs[\"input_ids\"].unsqueeze(0).to(model.config.device)\n",
        "    attention_mask = text_inputs[\"attention_mask\"].unsqueeze(0).to(model.config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "        predicted_idx = logits.argmax(dim=-1).item()\n",
        "\n",
        "    predicted_answer = answer_choices[predicted_idx]\n",
        "\n",
        "    print(f\"\\n  Image: {image_path}\")\n",
        "    print(f\" Question: {question}\")\n",
        "    print(f\"\\n Results:\")\n",
        "    for i, (ans, prob) in enumerate(zip(answer_choices, probs.cpu().tolist())):\n",
        "        marker = \"\" if i == predicted_idx else \"  \"\n",
        "        print(f\"  {marker} {i}: {ans} ({100*prob:.1f}%)\")\n",
        "    print(f\"\\n Predicted answer: {predicted_answer}\")\n",
        "\n",
        "    return {\n",
        "        \"predicted_idx\": predicted_idx,\n",
        "        \"predicted_answer\": predicted_answer,\n",
        "        \"probabilities\": probs.cpu().tolist(),\n",
        "        \"all_scores\": logits[0].cpu().tolist(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcfOs703cVkH"
      },
      "source": [
        "# Experiment Flow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY7ggAKbqIyZ"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(\n",
        "    dataset: GDVCRDataset, config: Config\n",
        ") -> Tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"Split dataset and create train/val data loaders.\"\"\"\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(total_size * config.val_split)\n",
        "    train_size = total_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(config.seed),\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset split:\")\n",
        "    print(f\"  Training samples: {len(train_dataset)}\")\n",
        "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if config.device == \"cuda\" else False,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if config.device == \"cuda\" else False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def run_experiment(config: Config) -> None:\n",
        "    \"\"\"Run the complete VPT experiment.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VISUAL PROMPT TUNING EXPERIMENT\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Model: {config.model_name}\")\n",
        "    print(f\"  Device: {config.device}\")\n",
        "    print(f\"  Prompt tokens: {config.num_prompt_tokens}\")\n",
        "    print(f\"  Learning rate: {config.learning_rate}\")\n",
        "    print(f\"  Batch size: {config.batch_size}\")\n",
        "    print(f\"  Epochs: {config.num_epochs}\")\n",
        "\n",
        "    print(\"\\n[Step 1] Setting random seed for reproducibility...\")\n",
        "    set_seed(config.seed)\n",
        "\n",
        "    print(\"\\n[Step 2] Loading CLIP processor...\")\n",
        "    processor = CLIPProcessor.from_pretrained(config.model_name)\n",
        "\n",
        "    print(\"\\n[Step 3] Loading GD-VCR dataset...\")\n",
        "    dataset = GDVCRDataset(\n",
        "        jsonl_path=config.dataset_path,\n",
        "        processor=processor,\n",
        "        config=config,\n",
        "        image_base_path=config.image_base_path,\n",
        "    )\n",
        "\n",
        "    print(\"\\n[Step 4] Creating data loaders...\")\n",
        "    train_loader, val_loader = create_data_loaders(dataset, config)\n",
        "\n",
        "    print(\"\\n[Step 5] Evaluating baseline CLIP (zero-shot)...\")\n",
        "    baseline_model = BaselineCLIP(config)\n",
        "    baseline_results = evaluate_model(\n",
        "        baseline_model, val_loader, config, \"Baseline CLIP\"\n",
        "    )\n",
        "    baseline_params = baseline_model.count_parameters()\n",
        "\n",
        "    del baseline_model\n",
        "    if config.device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n[Step 6] Training CLIP + VPT...\")\n",
        "    vpt_model = CLIPWithVPT(config)\n",
        "    vpt_trainable_params = vpt_model.count_trainable_parameters()\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=vpt_model, train_loader=train_loader, val_loader=val_loader, config=config\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(\"\\n[Step 7] Evaluating trained CLIP + VPT...\")\n",
        "    vpt_results = evaluate_model(vpt_model, val_loader, config, \"CLIP + VPT\")\n",
        "\n",
        "    print(\"\\n[Step 8] Comparing models...\")\n",
        "    compare_models(\n",
        "        baseline_results=baseline_results,\n",
        "        vpt_results=vpt_results,\n",
        "        baseline_params=baseline_params,\n",
        "        vpt_trainable_params=vpt_trainable_params,\n",
        "        vpt_training_time=training_time,\n",
        "    )\n",
        "\n",
        "    print(\"\\n TRAINING HISTORY SUMMARY:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Initial train acc: {100*history['train_acc'][0]:.2f}%\")\n",
        "    print(f\"  Final train acc:   {100*history['train_acc'][-1]:.2f}%\")\n",
        "    print(f\"  Initial val acc:   {100*history['val_acc'][0]:.2f}%\")\n",
        "    print(f\"  Final val acc:     {100*history['val_acc'][-1]:.2f}%\")\n",
        "\n",
        "    print(\"\\n[Step 9] Saving trained model...\")\n",
        "    save_path = \"clip_vpt_model.pt\"\n",
        "    save_model(vpt_model, save_path, config)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"EXPERIMENT COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return history, vpt_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9wK0Z87cRiV"
      },
      "source": [
        "# Main Func\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "su0lB3OSqI07"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main entry point for the VPT experiment.\"\"\"\n",
        "    config = Config()\n",
        "\n",
        "    try:\n",
        "        history, model = run_experiment(config)\n",
        "        print(\"\\n Experiment completed successfully!\")\n",
        "        return history, model\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\n Error: Dataset file not found: {e}\")\n",
        "        print(\"Please update the dataset_path in the Config class.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error during experiment: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Quick test function to verify the pipeline works with synthetic data.\"\"\"\n",
        "    print(\"Running quick test with synthetic data...\")\n",
        "\n",
        "    config = Config()\n",
        "    config.num_epochs = 1\n",
        "    config.batch_size = 2\n",
        "\n",
        "    synthetic_data = [\n",
        "        {\n",
        "            \"img_fn\": \"test.jpg\",\n",
        "            \"objects\": [\"person\", \"dog\", \"car\"],\n",
        "            \"question\": [\"What\", \"is\", [0], \"doing\", \"?\"],\n",
        "            \"answer_choices\": [\n",
        "                [\"Walking\", \"the\", [1], \".\"],\n",
        "                [\"Driving\", \"the\", [2], \".\"],\n",
        "                [\"Sleeping\", \".\"],\n",
        "                [\"Running\", \".\"],\n",
        "            ],\n",
        "            \"answer_label\": 0,\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    with open(\"test_data.jsonl\", \"w\") as f:\n",
        "        for item in synthetic_data:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "    test_image = Image.new(\"RGB\", (224, 224), color=\"white\")\n",
        "    test_image.save(\"test.jpg\")\n",
        "\n",
        "    print(\"\\nTesting preprocessing...\")\n",
        "    for record in synthetic_data:\n",
        "        processed = preprocess_sample(record)\n",
        "        print(f\"  Question: {processed['question']}\")\n",
        "        print(f\"  Answers: {processed['answer_choices']}\")\n",
        "\n",
        "    print(\"\\n Quick test passed! The pipeline is working.\")\n",
        "\n",
        "    os.remove(\"test_data.jsonl\")\n",
        "    os.remove(\"test.jpg\")\n",
        "\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies for Google Colab.\"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = [\"torch\", \"transformers\", \"Pillow\", \"tqdm\"]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "    print(\"\\n All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0qcHm1jqxEG"
      },
      "source": [
        "# RUN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "73d49784256b4b38826a35a38a9b851d",
            "ea2faf7f9e724a14a5e7256352017459",
            "4e1d1d53c5764d87831385f31d70a607",
            "d4b3711c42d646ef8d4edec38d87de18",
            "2f11ae0b9e384d64ad22554cc2f1518b",
            "2e1caba7c55c4eaa8ee4033181cedf6e",
            "e8671ccc6b0d4296a9a8dc1cb251d4a9",
            "81e4b5818d1b4e1ca95158c618daed4f",
            "de50dbd356da4c81b4731d5bc69613d1",
            "9df37d5760984bfca48d2ba17bbef9ef",
            "3ed78acdb9ea464ea8c2729de1cf4ca5",
            "9de24adb19e84e2bbf592e669b3aff1c",
            "89900135ced74ad49fc02bf2e89b56bb",
            "43f3e0ab5e4f43f6b66d05479dbc14a8",
            "ea25cc1805ca45dd9e548c2641592150",
            "0374c6b847d5471a9051120e9e46af8a",
            "89c81096cafd4adba949ad3a53488c74",
            "7d1b32a96b5b496c895e321ee830dde5",
            "030a34532eda42c9892c6012b9fd10b8",
            "d03ef747616b41bba5c8a14aadd06d9b",
            "2b88c0b7e3ea4f9ebd8616acebad58dc",
            "2647f4a024ea45b394d65b8258f3c9e8"
          ]
        },
        "id": "8QBtAUcCqI36",
        "outputId": "b0d63f76-38a5-49e5-811e-46513f79bf35"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKO7VIehqI6q"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Load & Test Prediction\n",
        "\n",
        "# Load the model\n",
        "model = load_model(\"clip_vpt_model.pt\")\n",
        "\n",
        "# Make a prediction\n",
        "result = predict(\n",
        "    model=model,\n",
        "    image_path=\"35.jpg\",\n",
        "    question=\"What is [person1] doing\",\n",
        "    answer_choices=[\"[person1] is just walking\", \"[person1] is hitting the drums at night\", \"[person1] is singing\", \"[person1] is dancing\"\n",
        "]\n",
        ")\n",
        "\n",
        "# Access results\n",
        "print(f\"Predicted: {result['predicted_answer']}\")\n",
        "print(f\"Confidence: {result['probabilities'][result['predicted_idx']]:.1%}\")\n",
        "\"\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "030a34532eda42c9892c6012b9fd10b8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0374c6b847d5471a9051120e9e46af8a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2647f4a024ea45b394d65b8258f3c9e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b88c0b7e3ea4f9ebd8616acebad58dc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e1caba7c55c4eaa8ee4033181cedf6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f11ae0b9e384d64ad22554cc2f1518b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ed78acdb9ea464ea8c2729de1cf4ca5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43f3e0ab5e4f43f6b66d05479dbc14a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_030a34532eda42c9892c6012b9fd10b8",
            "max": 590,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d03ef747616b41bba5c8a14aadd06d9b",
            "value": 590
          }
        },
        "4e1d1d53c5764d87831385f31d70a607": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81e4b5818d1b4e1ca95158c618daed4f",
            "max": 590,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de50dbd356da4c81b4731d5bc69613d1",
            "value": 590
          }
        },
        "73d49784256b4b38826a35a38a9b851d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ea2faf7f9e724a14a5e7256352017459",
              "IPY_MODEL_4e1d1d53c5764d87831385f31d70a607",
              "IPY_MODEL_d4b3711c42d646ef8d4edec38d87de18"
            ],
            "layout": "IPY_MODEL_2f11ae0b9e384d64ad22554cc2f1518b"
          }
        },
        "7d1b32a96b5b496c895e321ee830dde5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81e4b5818d1b4e1ca95158c618daed4f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89900135ced74ad49fc02bf2e89b56bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89c81096cafd4adba949ad3a53488c74",
            "placeholder": "​",
            "style": "IPY_MODEL_7d1b32a96b5b496c895e321ee830dde5",
            "value": "Loading weights: 100%"
          }
        },
        "89c81096cafd4adba949ad3a53488c74": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9de24adb19e84e2bbf592e669b3aff1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_89900135ced74ad49fc02bf2e89b56bb",
              "IPY_MODEL_43f3e0ab5e4f43f6b66d05479dbc14a8",
              "IPY_MODEL_ea25cc1805ca45dd9e548c2641592150"
            ],
            "layout": "IPY_MODEL_0374c6b847d5471a9051120e9e46af8a"
          }
        },
        "9df37d5760984bfca48d2ba17bbef9ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d03ef747616b41bba5c8a14aadd06d9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d4b3711c42d646ef8d4edec38d87de18": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9df37d5760984bfca48d2ba17bbef9ef",
            "placeholder": "​",
            "style": "IPY_MODEL_3ed78acdb9ea464ea8c2729de1cf4ca5",
            "value": " 590/590 [00:00&lt;00:00, 945.31it/s, Materializing param=visual_projection.weight]"
          }
        },
        "de50dbd356da4c81b4731d5bc69613d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8671ccc6b0d4296a9a8dc1cb251d4a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea25cc1805ca45dd9e548c2641592150": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b88c0b7e3ea4f9ebd8616acebad58dc",
            "placeholder": "​",
            "style": "IPY_MODEL_2647f4a024ea45b394d65b8258f3c9e8",
            "value": " 590/590 [00:00&lt;00:00, 772.20it/s, Materializing param=visual_projection.weight]"
          }
        },
        "ea2faf7f9e724a14a5e7256352017459": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e1caba7c55c4eaa8ee4033181cedf6e",
            "placeholder": "​",
            "style": "IPY_MODEL_e8671ccc6b0d4296a9a8dc1cb251d4a9",
            "value": "Loading weights: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
