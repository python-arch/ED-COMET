{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MOaAauiIpmr"
      },
      "source": [
        "# Imports\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxijsjY0REAh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from dataclasses import dataclass\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from transformers import CLIPProcessor, CLIPModel, CLIPVisionModel, CLIPTextModel\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "etspRDhTUwTm",
        "outputId": "891c0c04-90bf-4041-8851-6fdabc8a476e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Raw9AmDNLqXK"
      },
      "source": [
        "# CONFIGS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1ExGD8FUDow"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class Config:\n",
        "    \"\"\"Central configuration class for all hyperparameters and paths.\"\"\"\n",
        "\n",
        "    # Model\n",
        "    model_name: str = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "    # Dataset paths\n",
        "    dataset_path: str = \"/content/drive/MyDrive/GD-VCR-Dataset/images-vcr-drive.jsonl\"\n",
        "    image_base_path: str = \"/content/drive/MyDrive/GD-VCR-Dataset/MC-VCR_sample/\"\n",
        "\n",
        "    # VPT Hyperparameters\n",
        "    num_prompt_tokens: int = 50\n",
        "    prompt_dim: int = 1024\n",
        "    prompt_dropout: float = 0.1\n",
        "\n",
        "    # Training Hyperparameters\n",
        "    batch_size: int = 16\n",
        "    learning_rate: float = 1e-3\n",
        "    num_epochs: int = 10\n",
        "    warmup_steps: int = 100\n",
        "    weight_decay: float = 0.01\n",
        "\n",
        "    # System\n",
        "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    seed: int = 42\n",
        "    num_workers: int = 2\n",
        "\n",
        "    # Validation split\n",
        "    val_split: float = 0.3\n",
        "    max_text_length: int = 77\n",
        "\n",
        "\n",
        "def set_seed(seed: int) -> None:\n",
        "    \"\"\"Set random seeds for reproducibility.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # Best practice: deterministic mode for reproducibility\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccJD4M3qLB27"
      },
      "source": [
        "# Dataset Parsing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XCkvDH3yUHIG"
      },
      "outputs": [],
      "source": [
        "def load_jsonl(file_path: str) -> List[Dict]:\n",
        "    \"\"\"Load a JSONL file and return a list of dictionaries.\"\"\"\n",
        "    data = []\n",
        "    print(f\"Loading dataset from: {file_path}\")\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line_num, line in enumerate(f, 1):\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                try:\n",
        "                    record = json.loads(line)\n",
        "                    data.append(record)\n",
        "                except json.JSONDecodeError as e:\n",
        "                    print(f\"Warning: Could not parse line {line_num}: {e}\")\n",
        "                    continue\n",
        "\n",
        "    print(f\"Loaded {len(data)} records from {file_path}\")\n",
        "    return data\n",
        "\n",
        "\n",
        "def validate_record(record: Dict) -> bool:\n",
        "    \"\"\"Validate that a GD-VCR record has all required fields.\"\"\"\n",
        "    required_fields = [\"img_fn\", \"objects\", \"question\", \"answer_choices\", \"answer_label\"]\n",
        "\n",
        "    for field in required_fields:\n",
        "        if field not in record:\n",
        "            return False\n",
        "\n",
        "    if record[\"answer_label\"] < 0 or record[\"answer_label\"] >= len(record[\"answer_choices\"]):\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeX7j239LGJj"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8F9BF1Y1UKQN"
      },
      "outputs": [],
      "source": [
        "def resolve_object_reference(token: Any, objects: List[str]) -> str:\n",
        "    \"\"\"Resolve an object reference token to its actual object name.\"\"\"\n",
        "    if isinstance(token, list) and len(token) == 1 and isinstance(token[0], int):\n",
        "        index = token[0]\n",
        "        if 0 <= index < len(objects):\n",
        "            return objects[index]\n",
        "        else:\n",
        "            return f\"object{index}\"\n",
        "\n",
        "    if isinstance(token, int):\n",
        "        if 0 <= token < len(objects):\n",
        "            return objects[token]\n",
        "        else:\n",
        "            return f\"object{token}\"\n",
        "\n",
        "    return str(token)\n",
        "\n",
        "\n",
        "def tokens_to_sentence(tokens: List[Any], objects: List[str]) -> str:\n",
        "    \"\"\"Convert a list of tokens (with object references) into a natural sentence.\"\"\"\n",
        "    resolved_tokens = [resolve_object_reference(token, objects) for token in tokens]\n",
        "    sentence = \" \".join(resolved_tokens)\n",
        "\n",
        "    punctuation = [\".\", \",\", \"!\", \"?\", \"'\", '\"', \":\", \";\"]\n",
        "    for punct in punctuation:\n",
        "        sentence = sentence.replace(f\" {punct}\", punct)\n",
        "\n",
        "    sentence = sentence.replace(\" ' s\", \"'s\")\n",
        "    sentence = sentence.replace(\" 's\", \"'s\")\n",
        "    sentence = \" \".join(sentence.split())\n",
        "\n",
        "    return sentence\n",
        "\n",
        "\n",
        "def preprocess_sample(record: Dict) -> Dict:\n",
        "    \"\"\"Preprocess a single GD-VCR record into a format suitable for CLIP.\"\"\"\n",
        "    objects = record[\"objects\"]\n",
        "    question = tokens_to_sentence(record[\"question\"], objects)\n",
        "\n",
        "    answer_choices = []\n",
        "    for choice_tokens in record[\"answer_choices\"]:\n",
        "        choice_sentence = tokens_to_sentence(choice_tokens, objects)\n",
        "        answer_choices.append(choice_sentence)\n",
        "\n",
        "    return {\n",
        "        \"question\": question,\n",
        "        \"answer_choices\": answer_choices,\n",
        "        \"answer_label\": record[\"answer_label\"],\n",
        "        \"img_fn\": record[\"img_fn\"],\n",
        "        \"objects\": objects,\n",
        "    }\n",
        "\n",
        "\n",
        "def create_qa_text(question: str, answer: str) -> str:\n",
        "    \"\"\"Combine question and answer into a single text for CLIP encoding.\"\"\"\n",
        "    return f\"Question: {question} Answer: {answer}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0DPBgBrLOZo"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcBl4b2EUONH"
      },
      "outputs": [],
      "source": [
        "class GDVCRDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for GD-VCR visual commonsense reasoning.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        jsonl_path: str,\n",
        "        processor: CLIPProcessor,\n",
        "        config: Config,\n",
        "        image_base_path: Optional[str] = None,\n",
        "    ):\n",
        "        self.processor = processor\n",
        "        self.config = config\n",
        "        self.image_base_path = image_base_path or config.image_base_path\n",
        "\n",
        "        raw_data = load_jsonl(jsonl_path)\n",
        "\n",
        "        self.data = []\n",
        "        for record in raw_data:\n",
        "            if validate_record(record):\n",
        "                preprocessed = preprocess_sample(record)\n",
        "                self.data.append(preprocessed)\n",
        "            else:\n",
        "                print(f\"Warning: Skipping invalid record\")\n",
        "\n",
        "        print(f\"Dataset initialized with {len(self.data)} valid samples\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def load_image(self, img_fn: str) -> Image.Image:\n",
        "        \"\"\"Load an image from disk.\"\"\"\n",
        "        img_path = os.path.join(self.image_base_path, img_fn)\n",
        "\n",
        "        try:\n",
        "            image = Image.open(img_path).convert(\"RGB\")\n",
        "            return image\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Warning: Image not found: {img_path}\")\n",
        "            return Image.new(\"RGB\", (224, 224), color=\"gray\")\n",
        "        except Exception as e:\n",
        "            print(f\"Warning: Error loading {img_path}: {e}\")\n",
        "            return Image.new(\"RGB\", (224, 224), color=\"gray\")\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        image = self.load_image(sample[\"img_fn\"])\n",
        "        image_inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        pixel_values = image_inputs[\"pixel_values\"].squeeze(0)\n",
        "\n",
        "        question = sample[\"question\"]\n",
        "        answer_choices = sample[\"answer_choices\"]\n",
        "\n",
        "        qa_texts = []\n",
        "        for answer in answer_choices:\n",
        "            qa_text = create_qa_text(question, answer)\n",
        "            qa_texts.append(qa_text)\n",
        "\n",
        "        text_inputs = self.processor(\n",
        "            text=qa_texts,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=self.config.max_text_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            \"pixel_values\": pixel_values,\n",
        "            \"input_ids\": text_inputs[\"input_ids\"],\n",
        "            \"attention_mask\": text_inputs[\"attention_mask\"],\n",
        "            \"label\": torch.tensor(sample[\"answer_label\"], dtype=torch.long),\n",
        "            \"num_choices\": len(answer_choices),\n",
        "            \"img_fn\": sample[\"img_fn\"],\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch: List[Dict]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Custom collate function for batching GD-VCR samples with variable choice counts.\"\"\"\n",
        "    batch_size = len(batch)\n",
        "    max_choices = max(sample[\"num_choices\"] for sample in batch)\n",
        "    max_length = batch[0][\"input_ids\"].shape[1]\n",
        "\n",
        "    pixel_values = torch.stack([sample[\"pixel_values\"] for sample in batch])\n",
        "    labels = torch.stack([sample[\"label\"] for sample in batch])\n",
        "    num_choices = torch.tensor([sample[\"num_choices\"] for sample in batch])\n",
        "\n",
        "    input_ids = torch.zeros(batch_size, max_choices, max_length, dtype=torch.long)\n",
        "    attention_mask = torch.zeros(batch_size, max_choices, max_length, dtype=torch.long)\n",
        "\n",
        "    for i, sample in enumerate(batch):\n",
        "        n_choices = sample[\"num_choices\"]\n",
        "        input_ids[i, :n_choices] = sample[\"input_ids\"]\n",
        "        attention_mask[i, :n_choices] = sample[\"attention_mask\"]\n",
        "\n",
        "    img_fns = [sample[\"img_fn\"] for sample in batch]\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"num_choices\": num_choices,\n",
        "        \"img_fns\": img_fns,\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mlSAiBqLRrv"
      },
      "source": [
        "# Baseline CLIP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJ8a-nhMUVKd"
      },
      "outputs": [],
      "source": [
        "class BaselineCLIP(nn.Module):\n",
        "    \"\"\"Baseline CLIP model for visual commonsense reasoning (zero-shot, all frozen).\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "\n",
        "        print(f\"Loading CLIP model: {config.model_name}\")\n",
        "        self.clip = CLIPModel.from_pretrained(config.model_name)\n",
        "\n",
        "        # Freeze all parameters\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.config = config\n",
        "        self.to(config.device)\n",
        "\n",
        "        print(f\"Baseline CLIP loaded with {self.count_parameters():,} parameters (all frozen)\")\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def count_trainable_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def encode_image(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        image_outputs = self.clip.get_image_features(pixel_values=pixel_values)\n",
        "\n",
        "        if hasattr(image_outputs, \"pooler_output\"):\n",
        "            image_features = image_outputs.pooler_output\n",
        "        else:\n",
        "            image_features = image_outputs\n",
        "\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "        return image_features\n",
        "\n",
        "    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, num_choices, seq_len = input_ids.shape\n",
        "\n",
        "        input_ids_flat = input_ids.view(-1, seq_len)\n",
        "        attention_mask_flat = attention_mask.view(-1, seq_len)\n",
        "\n",
        "        text_outputs = self.clip.get_text_features(\n",
        "            input_ids=input_ids_flat, attention_mask=attention_mask_flat\n",
        "        )\n",
        "\n",
        "        if hasattr(text_outputs, \"pooler_output\"):\n",
        "            text_features = text_outputs.pooler_output\n",
        "        else:\n",
        "            text_features = text_outputs\n",
        "\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "        text_features = text_features.view(batch_size, num_choices, -1)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.Tensor,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        **kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        image_features = self.encode_image(pixel_values)\n",
        "        text_features = self.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        logits = torch.einsum(\"bcd,bcd->bc\", image_features.expand_as(text_features), text_features)\n",
        "\n",
        "        logit_scale = self.clip.logit_scale.exp()\n",
        "        logits = logits * logit_scale\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRPtngh3LUG8"
      },
      "source": [
        "# VPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KG9XyZjuUZ6r"
      },
      "outputs": [],
      "source": [
        "class VisualPromptTokens(nn.Module):\n",
        "    \"\"\"Learnable visual prompt tokens for VPT (prepended to image patch sequence).\"\"\"\n",
        "\n",
        "    def __init__(self, num_prompts: int, hidden_dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.num_prompts = num_prompts\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # Learnable prompts - only trainable parameters\n",
        "        self.prompts = nn.Parameter(torch.zeros(1, num_prompts, hidden_dim))\n",
        "\n",
        "        # Best practice: Xavier initialization for proper gradient flow\n",
        "        nn.init.xavier_uniform_(self.prompts)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        print(f\"Created {num_prompts} visual prompts with dim {hidden_dim}\")\n",
        "        print(f\"Prompt parameters: {num_prompts * hidden_dim:,}\")\n",
        "\n",
        "    def forward(self, batch_size: int) -> torch.Tensor:\n",
        "        prompts = self.prompts.expand(batch_size, -1, -1)\n",
        "        prompts = self.dropout(prompts)\n",
        "        return prompts\n",
        "\n",
        "\n",
        "class CLIPWithVPT(nn.Module):\n",
        "    \"\"\"CLIP model enhanced with Visual Prompt Tuning (VPT-Shallow).\"\"\"\n",
        "\n",
        "    def __init__(self, config: Config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        print(f\"Loading CLIP model for VPT: {config.model_name}\")\n",
        "        self.clip = CLIPModel.from_pretrained(config.model_name)\n",
        "\n",
        "        # Freeze CLIP backbone - only prompts are trainable\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.visual_prompts = VisualPromptTokens(\n",
        "            num_prompts=config.num_prompt_tokens,\n",
        "            hidden_dim=config.prompt_dim,\n",
        "            dropout=config.prompt_dropout,\n",
        "        )\n",
        "\n",
        "        self.to(config.device)\n",
        "\n",
        "        total_params = self.count_parameters()\n",
        "        trainable_params = self.count_trainable_parameters()\n",
        "        print(f\"Total parameters: {total_params:,}\")\n",
        "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"Trainable ratio: {100*trainable_params/total_params:.4f}%\")\n",
        "\n",
        "    def count_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters())\n",
        "\n",
        "    def count_trainable_parameters(self) -> int:\n",
        "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
        "\n",
        "    def get_visual_embeddings_with_prompts(self, pixel_values: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Get visual embeddings with prompts injected after [CLS] token.\"\"\"\n",
        "        batch_size = pixel_values.shape[0]\n",
        "        vision_model = self.clip.vision_model\n",
        "\n",
        "        embeddings = vision_model.embeddings(pixel_values)\n",
        "\n",
        "        cls_token = embeddings[:, :1, :]\n",
        "        patch_embeddings = embeddings[:, 1:, :]\n",
        "\n",
        "        prompts = self.visual_prompts(batch_size)\n",
        "\n",
        "        # Inject prompts: [CLS] + [Prompts] + [Patches]\n",
        "        embeddings_with_prompts = torch.cat([cls_token, prompts, patch_embeddings], dim=1)\n",
        "\n",
        "        hidden_states = embeddings_with_prompts\n",
        "        hidden_states = vision_model.pre_layrnorm(hidden_states)\n",
        "\n",
        "        encoder_outputs = vision_model.encoder(\n",
        "            inputs_embeds=hidden_states,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "        )\n",
        "\n",
        "        if isinstance(encoder_outputs, tuple):\n",
        "            hidden_states = encoder_outputs[0]\n",
        "        else:\n",
        "            hidden_states = encoder_outputs.last_hidden_state\n",
        "\n",
        "        pooled_output = hidden_states[:, 0, :]\n",
        "        pooled_output = vision_model.post_layernorm(pooled_output)\n",
        "\n",
        "        image_features = self.clip.visual_projection(pooled_output)\n",
        "        image_features = F.normalize(image_features, dim=-1)\n",
        "\n",
        "        return image_features\n",
        "\n",
        "    def encode_text(self, input_ids: torch.Tensor, attention_mask: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, num_choices, seq_len = input_ids.shape\n",
        "\n",
        "        input_ids_flat = input_ids.view(-1, seq_len)\n",
        "        attention_mask_flat = attention_mask.view(-1, seq_len)\n",
        "\n",
        "        text_output = self.clip.get_text_features(\n",
        "            input_ids=input_ids_flat, attention_mask=attention_mask_flat\n",
        "        )\n",
        "\n",
        "        if hasattr(text_output, 'pooler_output'):\n",
        "            text_features = text_output.pooler_output\n",
        "        elif hasattr(text_output, 'last_hidden_state'):\n",
        "            text_features = text_output.last_hidden_state[:, 0, :]\n",
        "        elif isinstance(text_output, torch.Tensor):\n",
        "            text_features = text_output\n",
        "        else:\n",
        "            text_features = text_output[0] if isinstance(text_output, tuple) else text_output\n",
        "\n",
        "        text_features = F.normalize(text_features, dim=-1)\n",
        "        text_features = text_features.view(batch_size, num_choices, -1)\n",
        "\n",
        "        return text_features\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        pixel_values: torch.Tensor,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        **kwargs,\n",
        "    ) -> torch.Tensor:\n",
        "        image_features = self.get_visual_embeddings_with_prompts(pixel_values)\n",
        "        text_features = self.encode_text(input_ids, attention_mask)\n",
        "\n",
        "        image_features = image_features.unsqueeze(1)\n",
        "        logits = torch.einsum(\"bcd,bcd->bc\", image_features.expand_as(text_features), text_features)\n",
        "\n",
        "        logit_scale = self.clip.logit_scale.exp()\n",
        "        logits = logits * logit_scale\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRfHUkfmLXqB"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvCqeA3YUeB3"
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    \"\"\"Training manager for VPT models.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        train_loader: DataLoader,\n",
        "        val_loader: DataLoader,\n",
        "        config: Config,\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.train_loader = train_loader\n",
        "        self.val_loader = val_loader\n",
        "        self.config = config\n",
        "\n",
        "        # Only optimize trainable parameters\n",
        "        trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "        self.optimizer = AdamW(trainable_params, lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        total_steps = len(train_loader) * config.num_epochs\n",
        "\n",
        "        def lr_lambda(step):\n",
        "            if step < config.warmup_steps:\n",
        "                return float(step) / float(max(1, config.warmup_steps))\n",
        "            return 1.0\n",
        "\n",
        "        self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda)\n",
        "\n",
        "        self.global_step = 0\n",
        "        self.best_val_accuracy = 0.0\n",
        "\n",
        "        print(f\"Trainer initialized:\")\n",
        "        print(f\"  - Training samples: {len(train_loader.dataset)}\")\n",
        "        print(f\"  - Validation samples: {len(val_loader.dataset)}\")\n",
        "        print(f\"  - Epochs: {config.num_epochs}\")\n",
        "        print(f\"  - Batch size: {config.batch_size}\")\n",
        "        print(f\"  - Learning rate: {config.learning_rate}\")\n",
        "\n",
        "    def train_epoch(self, epoch: int) -> Dict[str, float]:\n",
        "        self.model.train()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(\n",
        "            self.train_loader,\n",
        "            desc=f\"Epoch {epoch+1}/{self.config.num_epochs}\",\n",
        "            leave=True,\n",
        "        )\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            pixel_values = batch[\"pixel_values\"].to(self.config.device)\n",
        "            input_ids = batch[\"input_ids\"].to(self.config.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.config.device)\n",
        "            labels = batch[\"labels\"].to(self.config.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            logits = self.model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            loss = self.criterion(logits, labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Best practice: gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(\n",
        "                [p for p in self.model.parameters() if p.requires_grad], max_norm=1.0\n",
        "            )\n",
        "\n",
        "            self.optimizer.step()\n",
        "            self.scheduler.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            self.global_step += 1\n",
        "\n",
        "            progress_bar.set_postfix(\n",
        "                {\"loss\": f\"{loss.item():.4f}\", \"acc\": f\"{100*correct/total:.2f}%\"}\n",
        "            )\n",
        "\n",
        "        avg_loss = total_loss / len(self.train_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def validate(self) -> Dict[str, float]:\n",
        "        self.model.eval()\n",
        "\n",
        "        total_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch in tqdm(self.val_loader, desc=\"Validating\", leave=False):\n",
        "            pixel_values = batch[\"pixel_values\"].to(self.config.device)\n",
        "            input_ids = batch[\"input_ids\"].to(self.config.device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.config.device)\n",
        "            labels = batch[\"labels\"].to(self.config.device)\n",
        "\n",
        "            logits = self.model(\n",
        "                pixel_values=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "            )\n",
        "\n",
        "            loss = self.criterion(logits, labels)\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            predictions = logits.argmax(dim=-1)\n",
        "            correct += (predictions == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        avg_loss = total_loss / len(self.val_loader)\n",
        "        accuracy = correct / total\n",
        "\n",
        "        return {\"loss\": avg_loss, \"accuracy\": accuracy}\n",
        "\n",
        "    def train(self) -> Dict[str, List[float]]:\n",
        "        history = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": []}\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"STARTING TRAINING\")\n",
        "        print(\"=\" * 60 + \"\\n\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            train_metrics = self.train_epoch(epoch)\n",
        "            val_metrics = self.validate()\n",
        "\n",
        "            print(f\"\\nEpoch {epoch+1}/{self.config.num_epochs}:\")\n",
        "            print(f\"  Train Loss: {train_metrics['loss']:.4f}, Train Acc: {100*train_metrics['accuracy']:.2f}%\")\n",
        "            print(f\"  Val Loss: {val_metrics['loss']:.4f}, Val Acc: {100*val_metrics['accuracy']:.2f}%\")\n",
        "\n",
        "            if val_metrics[\"accuracy\"] > self.best_val_accuracy:\n",
        "                self.best_val_accuracy = val_metrics[\"accuracy\"]\n",
        "                print(f\"  → New best validation accuracy: {100*self.best_val_accuracy:.2f}%\")\n",
        "\n",
        "            history[\"train_loss\"].append(train_metrics[\"loss\"])\n",
        "            history[\"train_acc\"].append(train_metrics[\"accuracy\"])\n",
        "            history[\"val_loss\"].append(val_metrics[\"loss\"])\n",
        "            history[\"val_acc\"].append(val_metrics[\"accuracy\"])\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"TRAINING COMPLETE\")\n",
        "        print(\"=\" * 60)\n",
        "        print(f\"Total time: {total_time/60:.2f} minutes\")\n",
        "        print(f\"Best validation accuracy: {100*self.best_val_accuracy:.2f}%\")\n",
        "\n",
        "        return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR49Q_QoLafP"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ88KAv7UhfW"
      },
      "outputs": [],
      "source": [
        "REGION_NAMES = [\"East Asia\", \"South Asia\", \"Africa\", \"West\"]\n",
        "\n",
        "\n",
        "def get_region_from_filename(img_fn: str) -> Optional[str]:\n",
        "    \"\"\"Infer cultural region from image filename prefix.\"\"\"\n",
        "    import re\n",
        "\n",
        "    filename = os.path.basename(img_fn)\n",
        "\n",
        "    if filename.startswith(\"jpn_\") or filename.startswith(\"kor_\"):\n",
        "        return \"East Asia\"\n",
        "    if filename.startswith(\"af_\"):\n",
        "        return \"Africa\"\n",
        "    if filename.startswith(\"sa_\"):\n",
        "        return \"South Asia\"\n",
        "    if filename.startswith(\"west_\"):\n",
        "        return \"West\"\n",
        "    if re.match(r\"^\\d+\\.jpe?g$\", filename, re.IGNORECASE):\n",
        "        return \"East Asia\"\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(\n",
        "    model: nn.Module, data_loader: DataLoader, config: Config, model_name: str = \"Model\"\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Evaluate a model on a dataset with per-region accuracy tracking.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    region_metrics = {region: {\"correct\": 0, \"total\": 0} for region in REGION_NAMES}\n",
        "\n",
        "    print(f\"\\nEvaluating {model_name}...\")\n",
        "\n",
        "    for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "        pixel_values = batch[\"pixel_values\"].to(config.device)\n",
        "        input_ids = batch[\"input_ids\"].to(config.device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(config.device)\n",
        "        labels = batch[\"labels\"].to(config.device)\n",
        "        img_fns = batch[\"img_fns\"]\n",
        "\n",
        "        logits = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        predictions = logits.argmax(dim=-1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        all_predictions.extend(predictions.cpu().tolist())\n",
        "        all_labels.extend(labels.cpu().tolist())\n",
        "\n",
        "        predictions_list = predictions.cpu().tolist()\n",
        "        labels_list = labels.cpu().tolist()\n",
        "\n",
        "        for i, img_fn in enumerate(img_fns):\n",
        "            region = get_region_from_filename(img_fn)\n",
        "            if region is not None:\n",
        "                region_metrics[region][\"total\"] += 1\n",
        "                if predictions_list[i] == labels_list[i]:\n",
        "                    region_metrics[region][\"correct\"] += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "\n",
        "    region_accuracies = {}\n",
        "    for region in REGION_NAMES:\n",
        "        if region_metrics[region][\"total\"] > 0:\n",
        "            region_accuracies[region] = {\n",
        "                \"accuracy\": region_metrics[region][\"correct\"] / region_metrics[region][\"total\"],\n",
        "                \"correct\": region_metrics[region][\"correct\"],\n",
        "                \"total\": region_metrics[region][\"total\"],\n",
        "            }\n",
        "        else:\n",
        "            region_accuracies[region] = {\"accuracy\": 0.0, \"correct\": 0, \"total\": 0}\n",
        "\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"  Overall Accuracy: {100*accuracy:.2f}%\")\n",
        "    print(f\"  Correct: {correct}/{total}\")\n",
        "    print(f\"\\n  Accuracy by Region:\")\n",
        "    for region in REGION_NAMES:\n",
        "        r = region_accuracies[region]\n",
        "        if r[\"total\"] > 0:\n",
        "            print(f\"  - {region}: {100*r['accuracy']:.2f}% ({r['correct']}/{r['total']})\")\n",
        "        else:\n",
        "            print(f\"  - {region}: N/A (0 samples)\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"correct\": correct,\n",
        "        \"total\": total,\n",
        "        \"predictions\": all_predictions,\n",
        "        \"labels\": all_labels,\n",
        "        \"region_accuracies\": region_accuracies,\n",
        "    }\n",
        "\n",
        "\n",
        "def compare_models(\n",
        "    baseline_results: Dict,\n",
        "    vpt_results: Dict,\n",
        "    baseline_params: int,\n",
        "    vpt_trainable_params: int,\n",
        "    vpt_training_time: float,\n",
        ") -> None:\n",
        "    \"\"\"Print a comparison between baseline and VPT models.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"MODEL COMPARISON: BASELINE CLIP vs CLIP + VPT\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    print(\"\\n OVERALL ACCURACY COMPARISON:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline CLIP (Zero-shot): {100*baseline_results['accuracy']:.2f}%\")\n",
        "    print(f\"  CLIP + VPT (Trained):      {100*vpt_results['accuracy']:.2f}%\")\n",
        "\n",
        "    improvement = vpt_results[\"accuracy\"] - baseline_results[\"accuracy\"]\n",
        "    print(f\"\\n  Improvement: {100*improvement:+.2f}%\")\n",
        "\n",
        "    print(\"\\n ACCURACY BY REGION:\")\n",
        "    print(\"-\" * 70)\n",
        "    print(f\"  {'Region':<15} {'Baseline CLIP':<25} {'CLIP + VPT':<25} {'Δ'}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    baseline_regions = baseline_results.get(\"region_accuracies\", {})\n",
        "    vpt_regions = vpt_results.get(\"region_accuracies\", {})\n",
        "\n",
        "    for region in REGION_NAMES:\n",
        "        b_acc = baseline_regions.get(region, {}).get(\"accuracy\", 0.0)\n",
        "        b_correct = baseline_regions.get(region, {}).get(\"correct\", 0)\n",
        "        b_total = baseline_regions.get(region, {}).get(\"total\", 0)\n",
        "\n",
        "        v_acc = vpt_regions.get(region, {}).get(\"accuracy\", 0.0)\n",
        "        v_correct = vpt_regions.get(region, {}).get(\"correct\", 0)\n",
        "        v_total = vpt_regions.get(region, {}).get(\"total\", 0)\n",
        "\n",
        "        if b_total > 0 or v_total > 0:\n",
        "            b_str = f\"{100*b_acc:.2f}% ({b_correct}/{b_total})\" if b_total > 0 else \"N/A\"\n",
        "            v_str = f\"{100*v_acc:.2f}% ({v_correct}/{v_total})\" if v_total > 0 else \"N/A\"\n",
        "            delta = v_acc - b_acc if (b_total > 0 and v_total > 0) else 0.0\n",
        "            delta_str = f\"{100*delta:+.2f}%\" if (b_total > 0 and v_total > 0) else \"N/A\"\n",
        "            print(f\"  {region:<15} {b_str:<25} {v_str:<25} {delta_str}\")\n",
        "        else:\n",
        "            print(f\"  {region:<15} {'N/A':<25} {'N/A':<25} {'N/A'}\")\n",
        "\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    print(\"\\n PARAMETER COMPARISON:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline trainable params: 0 (all frozen)\")\n",
        "    print(f\"  VPT trainable params:      {vpt_trainable_params:,}\")\n",
        "    print(f\"  Trainable ratio:           {100*vpt_trainable_params/baseline_params:.4f}%\")\n",
        "\n",
        "    print(\"\\n TRAINING TIME:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Baseline: 0 (no training)\")\n",
        "    print(f\"  VPT:      {vpt_training_time/60:.2f} minutes\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "\n",
        "    print(\"\\n INTERPRETATION:\")\n",
        "    if improvement > 0:\n",
        "        print(f\"  → VPT improved accuracy by {100*improvement:.2f}% over zero-shot CLIP\")\n",
        "        print(f\"  → This was achieved by training only {vpt_trainable_params:,} parameters\")\n",
        "        print(f\"  → VPT is {baseline_params/vpt_trainable_params:.0f}x more parameter-efficient than full fine-tuning\")\n",
        "    else:\n",
        "        print(f\"  → VPT did not improve over baseline in this experiment\")\n",
        "        print(f\"  → Consider: adjusting prompt count, learning rate, or training longer\")\n",
        "\n",
        "\n",
        "def save_model(model: CLIPWithVPT, save_path: str, config: Config) -> None:\n",
        "    \"\"\"Save the trained VPT model (only prompt tokens, backbone is frozen).\"\"\"\n",
        "    save_dir = os.path.dirname(save_path)\n",
        "    if save_dir and not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    checkpoint = {\n",
        "        \"visual_prompts\": model.visual_prompts.state_dict(),\n",
        "        \"config\": {\n",
        "            \"model_name\": config.model_name,\n",
        "            \"num_prompt_tokens\": config.num_prompt_tokens,\n",
        "            \"prompt_dim\": config.prompt_dim,\n",
        "            \"prompt_dropout\": config.prompt_dropout,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"\\n Model saved to: {save_path}\")\n",
        "    print(f\"   Saved prompt parameters: {model.count_trainable_parameters():,}\")\n",
        "\n",
        "\n",
        "def load_model(load_path: str, config: Optional[Config] = None) -> CLIPWithVPT:\n",
        "    \"\"\"Load a saved VPT model.\"\"\"\n",
        "    checkpoint = torch.load(load_path, map_location=\"cpu\")\n",
        "\n",
        "    if config is None:\n",
        "        config = Config()\n",
        "        saved_config = checkpoint[\"config\"]\n",
        "        config.model_name = saved_config[\"model_name\"]\n",
        "        config.num_prompt_tokens = saved_config[\"num_prompt_tokens\"]\n",
        "        config.prompt_dim = saved_config[\"prompt_dim\"]\n",
        "        config.prompt_dropout = saved_config[\"prompt_dropout\"]\n",
        "\n",
        "    model = CLIPWithVPT(config)\n",
        "    model.visual_prompts.load_state_dict(checkpoint[\"visual_prompts\"])\n",
        "\n",
        "    print(f\"\\n Model loaded from: {load_path}\")\n",
        "    print(f\"   Loaded prompt parameters: {model.count_trainable_parameters():,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def predict(\n",
        "    model: CLIPWithVPT,\n",
        "    image_path: str,\n",
        "    question: str,\n",
        "    answer_choices: List[str],\n",
        "    processor: Optional[CLIPProcessor] = None,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Make a prediction using the trained VPT model.\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if processor is None:\n",
        "        processor = CLIPProcessor.from_pretrained(model.config.model_name)\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image_inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    pixel_values = image_inputs[\"pixel_values\"].to(model.config.device)\n",
        "\n",
        "    qa_texts = [f\"Question: {question} Answer: {ans}\" for ans in answer_choices]\n",
        "\n",
        "    text_inputs = processor(\n",
        "        text=qa_texts,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=77,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "\n",
        "    input_ids = text_inputs[\"input_ids\"].unsqueeze(0).to(model.config.device)\n",
        "    attention_mask = text_inputs[\"attention_mask\"].unsqueeze(0).to(model.config.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "        )\n",
        "\n",
        "        probs = torch.softmax(logits, dim=-1)[0]\n",
        "        predicted_idx = logits.argmax(dim=-1).item()\n",
        "\n",
        "    predicted_answer = answer_choices[predicted_idx]\n",
        "\n",
        "    print(f\"\\n  Image: {image_path}\")\n",
        "    print(f\" Question: {question}\")\n",
        "    print(f\"\\n Results:\")\n",
        "    for i, (ans, prob) in enumerate(zip(answer_choices, probs.cpu().tolist())):\n",
        "        marker = \"\" if i == predicted_idx else \"  \"\n",
        "        print(f\"  {marker} {i}: {ans} ({100*prob:.1f}%)\")\n",
        "    print(f\"\\n Predicted answer: {predicted_answer}\")\n",
        "\n",
        "    return {\n",
        "        \"predicted_idx\": predicted_idx,\n",
        "        \"predicted_answer\": predicted_answer,\n",
        "        \"probabilities\": probs.cpu().tolist(),\n",
        "        \"all_scores\": logits[0].cpu().tolist(),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJFqVJNOLfjh"
      },
      "source": [
        "# Experiment Flow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VMFWD7gaUmAo"
      },
      "outputs": [],
      "source": [
        "def create_data_loaders(dataset: GDVCRDataset, config: Config) -> Tuple[DataLoader, DataLoader]:\n",
        "    \"\"\"Split dataset and create train/val data loaders.\"\"\"\n",
        "    total_size = len(dataset)\n",
        "    val_size = int(total_size * config.val_split)\n",
        "    train_size = total_size - val_size\n",
        "\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(\n",
        "        dataset,\n",
        "        [train_size, val_size],\n",
        "        generator=torch.Generator().manual_seed(config.seed),\n",
        "    )\n",
        "\n",
        "    print(f\"\\nDataset split:\")\n",
        "    print(f\"  Training samples: {len(train_dataset)}\")\n",
        "    print(f\"  Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=config.num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if config.device == \"cuda\" else False,\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=config.batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=config.num_workers,\n",
        "        collate_fn=collate_fn,\n",
        "        pin_memory=True if config.device == \"cuda\" else False,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def run_experiment(config: Config) -> None:\n",
        "    \"\"\"Run the complete VPT experiment.\"\"\"\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"VISUAL PROMPT TUNING EXPERIMENT\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Model: {config.model_name}\")\n",
        "    print(f\"  Device: {config.device}\")\n",
        "    print(f\"  Prompt tokens: {config.num_prompt_tokens}\")\n",
        "    print(f\"  Learning rate: {config.learning_rate}\")\n",
        "    print(f\"  Batch size: {config.batch_size}\")\n",
        "    print(f\"  Epochs: {config.num_epochs}\")\n",
        "\n",
        "    print(\"\\n[Step 1] Setting random seed for reproducibility...\")\n",
        "    set_seed(config.seed)\n",
        "\n",
        "    print(\"\\n[Step 2] Loading CLIP processor...\")\n",
        "    processor = CLIPProcessor.from_pretrained(config.model_name)\n",
        "\n",
        "    print(\"\\n[Step 3] Loading GD-VCR dataset...\")\n",
        "    dataset = GDVCRDataset(\n",
        "        jsonl_path=config.dataset_path,\n",
        "        processor=processor,\n",
        "        config=config,\n",
        "        image_base_path=config.image_base_path,\n",
        "    )\n",
        "\n",
        "    print(\"\\n[Step 4] Creating data loaders...\")\n",
        "    train_loader, val_loader = create_data_loaders(dataset, config)\n",
        "\n",
        "    print(\"\\n[Step 5] Evaluating baseline CLIP (zero-shot)...\")\n",
        "    baseline_model = BaselineCLIP(config)\n",
        "    baseline_results = evaluate_model(baseline_model, val_loader, config, \"Baseline CLIP\")\n",
        "    baseline_params = baseline_model.count_parameters()\n",
        "\n",
        "    del baseline_model\n",
        "    if config.device == \"cuda\":\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    print(\"\\n[Step 6] Training CLIP + VPT...\")\n",
        "    vpt_model = CLIPWithVPT(config)\n",
        "    vpt_trainable_params = vpt_model.count_trainable_parameters()\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=vpt_model, train_loader=train_loader, val_loader=val_loader, config=config\n",
        "    )\n",
        "\n",
        "    start_time = time.time()\n",
        "    history = trainer.train()\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    print(\"\\n[Step 7] Evaluating trained CLIP + VPT...\")\n",
        "    vpt_results = evaluate_model(vpt_model, val_loader, config, \"CLIP + VPT\")\n",
        "\n",
        "    print(\"\\n[Step 8] Comparing models...\")\n",
        "    compare_models(\n",
        "        baseline_results=baseline_results,\n",
        "        vpt_results=vpt_results,\n",
        "        baseline_params=baseline_params,\n",
        "        vpt_trainable_params=vpt_trainable_params,\n",
        "        vpt_training_time=training_time,\n",
        "    )\n",
        "\n",
        "    print(\"\\n TRAINING HISTORY SUMMARY:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"  Initial train acc: {100*history['train_acc'][0]:.2f}%\")\n",
        "    print(f\"  Final train acc:   {100*history['train_acc'][-1]:.2f}%\")\n",
        "    print(f\"  Initial val acc:   {100*history['val_acc'][0]:.2f}%\")\n",
        "    print(f\"  Final val acc:     {100*history['val_acc'][-1]:.2f}%\")\n",
        "\n",
        "    print(\"\\n[Step 9] Saving trained model...\")\n",
        "    save_path = \"clip_vpt_model.pt\"\n",
        "    save_model(vpt_model, save_path, config)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"EXPERIMENT COMPLETE\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    return history, vpt_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT-m015JLjAL"
      },
      "source": [
        "# Main Func"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGmcqyXuUoh6"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main entry point for the VPT experiment.\"\"\"\n",
        "    config = Config()\n",
        "\n",
        "    try:\n",
        "        history, model = run_experiment(config)\n",
        "        print(\"\\n Experiment completed successfully!\")\n",
        "        return history, model\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"\\n Error: Dataset file not found: {e}\")\n",
        "        print(\"Please update the dataset_path in the Config class.\")\n",
        "        raise\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error during experiment: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def quick_test():\n",
        "    \"\"\"Quick test function to verify the pipeline works with synthetic data.\"\"\"\n",
        "    print(\"Running quick test with synthetic data...\")\n",
        "\n",
        "    config = Config()\n",
        "    config.num_epochs = 1\n",
        "    config.batch_size = 2\n",
        "\n",
        "    synthetic_data = [\n",
        "        {\n",
        "            \"img_fn\": \"test.jpg\",\n",
        "            \"objects\": [\"person\", \"dog\", \"car\"],\n",
        "            \"question\": [\"What\", \"is\", [0], \"doing\", \"?\"],\n",
        "            \"answer_choices\": [\n",
        "                [\"Walking\", \"the\", [1], \".\"],\n",
        "                [\"Driving\", \"the\", [2], \".\"],\n",
        "                [\"Sleeping\", \".\"],\n",
        "                [\"Running\", \".\"],\n",
        "            ],\n",
        "            \"answer_label\": 0,\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    with open(\"test_data.jsonl\", \"w\") as f:\n",
        "        for item in synthetic_data:\n",
        "            f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "    test_image = Image.new(\"RGB\", (224, 224), color=\"white\")\n",
        "    test_image.save(\"test.jpg\")\n",
        "\n",
        "    print(\"\\nTesting preprocessing...\")\n",
        "    for record in synthetic_data:\n",
        "        processed = preprocess_sample(record)\n",
        "        print(f\"  Question: {processed['question']}\")\n",
        "        print(f\"  Answers: {processed['answer_choices']}\")\n",
        "\n",
        "    print(\"\\n Quick test passed! The pipeline is working.\")\n",
        "\n",
        "    os.remove(\"test_data.jsonl\")\n",
        "    os.remove(\"test.jpg\")\n",
        "\n",
        "\n",
        "def install_dependencies():\n",
        "    \"\"\"Install required dependencies for Google Colab.\"\"\"\n",
        "    import subprocess\n",
        "    import sys\n",
        "\n",
        "    packages = [\"torch\", \"transformers\", \"Pillow\", \"tqdm\"]\n",
        "\n",
        "    for package in packages:\n",
        "        print(f\"Installing {package}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
        "\n",
        "    print(\"\\n All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYVypQVtLmNb"
      },
      "source": [
        "# RUN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0c8c800e89b44c618e45ecbc792dad47",
            "c0593deb81674c96ad25ad7c481b5152",
            "8c30df575597433a95947c62c1542780",
            "38b17f052feb40a19d47d31bde143039",
            "66dbdbff70cd4efd9d04406d5d4cb10f",
            "fa5802d6f6234af99090fb47aab34c03",
            "c42ae601764a4ab2842e88faff232581",
            "76bbeffe40e74ce2a395fefcb44aea95",
            "491199489732400e97470a7ad5281b4e",
            "0c60b18cdb094a11a8b1389c5905c0b1",
            "848797f9ddd748afa09bf1c3c67874d6",
            "ba1e67e6529f4895961f6657f8c3ee91",
            "bff3ba233e494a389f439965f2f5ae5e",
            "2df28c6655164aa09452657827a4a67c",
            "6dd4c0ca40674edb8febfec250159b3f",
            "fd2f5996a2714a789dc7f0e8df5c0f93",
            "e1d52121b02648bfa379bb5a6538a455",
            "06c0f02297f545918bd87eab48d866f1",
            "55d1654b1d514f4bb699fe210a1fb585",
            "3914b53c8c3b499cb459657bf3d226c8",
            "5a2866b3900247e786bbbdda99c41e70",
            "c2c40671a93a4f3e8d085744423206cd"
          ]
        },
        "id": "hGp0HLkAUq3I",
        "outputId": "046ac4d0-3f18-4f6f-e101-f3f154e2d10b"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# RUN THE EXPERIMENT\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w8npffSjqMv"
      },
      "source": [
        "# Load & Test Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 344,
          "referenced_widgets": [
            "ab56ae55efe746c7bcfc041e48bd1410",
            "8899531cfacd47dabf06c000625b126f",
            "a1ee093494a841bd950bffde7dddcbf4",
            "5069bc1529db46e9850320b73c7c1d9c",
            "50bb2c8a422a44f59da31ff110b98793",
            "61d1f3d3449d4d2f9cf3397245d61666",
            "c333be4f6eee4613b3774602694082a9",
            "4d00f1a86d8145bfba4bc617522717e3",
            "5a7247a3dc29458bb5b64a2ac8349de8",
            "836b79cda39d45349d0555ad1707f793",
            "fceb0e1010a74a51862e7b7a2a8ebe9d"
          ]
        },
        "id": "LillIAttjmcv",
        "outputId": "b847fcc4-186f-4add-abaf-a02f0cd826e1"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Load the model\n",
        "model = load_model(\"clip_vpt_model.pt\")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L5rFRueglyS4"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Make a prediction\n",
        "result = predict(\n",
        "    model=model,\n",
        "    image_path=\"35.jpg\",\n",
        "    question=\"What is [person1] doing\",\n",
        "    answer_choices=[\"[person1] is just walking\", \"[person1] is hitting the drums at night\", \"[person1] is singing\", \"[person1] is dancing\"\n",
        "]\n",
        ")\n",
        "\n",
        "# Access results\n",
        "print(f\"Predicted: {result['predicted_answer']}\")\n",
        "print(f\"Confidence: {result['probabilities'][result['predicted_idx']]:.1%}\")\n",
        "\"\"\"\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06c0f02297f545918bd87eab48d866f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0c60b18cdb094a11a8b1389c5905c0b1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c8c800e89b44c618e45ecbc792dad47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0593deb81674c96ad25ad7c481b5152",
              "IPY_MODEL_8c30df575597433a95947c62c1542780",
              "IPY_MODEL_38b17f052feb40a19d47d31bde143039"
            ],
            "layout": "IPY_MODEL_66dbdbff70cd4efd9d04406d5d4cb10f"
          }
        },
        "2df28c6655164aa09452657827a4a67c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55d1654b1d514f4bb699fe210a1fb585",
            "max": 590,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3914b53c8c3b499cb459657bf3d226c8",
            "value": 590
          }
        },
        "38b17f052feb40a19d47d31bde143039": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c60b18cdb094a11a8b1389c5905c0b1",
            "placeholder": "​",
            "style": "IPY_MODEL_848797f9ddd748afa09bf1c3c67874d6",
            "value": " 590/590 [00:00&lt;00:00, 861.64it/s, Materializing param=visual_projection.weight]"
          }
        },
        "3914b53c8c3b499cb459657bf3d226c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "491199489732400e97470a7ad5281b4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d00f1a86d8145bfba4bc617522717e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5069bc1529db46e9850320b73c7c1d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_836b79cda39d45349d0555ad1707f793",
            "placeholder": "​",
            "style": "IPY_MODEL_fceb0e1010a74a51862e7b7a2a8ebe9d",
            "value": " 590/590 [00:00&lt;00:00, 842.60it/s, Materializing param=visual_projection.weight]"
          }
        },
        "50bb2c8a422a44f59da31ff110b98793": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55d1654b1d514f4bb699fe210a1fb585": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2866b3900247e786bbbdda99c41e70": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a7247a3dc29458bb5b64a2ac8349de8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61d1f3d3449d4d2f9cf3397245d61666": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66dbdbff70cd4efd9d04406d5d4cb10f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dd4c0ca40674edb8febfec250159b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a2866b3900247e786bbbdda99c41e70",
            "placeholder": "​",
            "style": "IPY_MODEL_c2c40671a93a4f3e8d085744423206cd",
            "value": " 590/590 [00:00&lt;00:00, 818.50it/s, Materializing param=visual_projection.weight]"
          }
        },
        "76bbeffe40e74ce2a395fefcb44aea95": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "836b79cda39d45349d0555ad1707f793": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "848797f9ddd748afa09bf1c3c67874d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8899531cfacd47dabf06c000625b126f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61d1f3d3449d4d2f9cf3397245d61666",
            "placeholder": "​",
            "style": "IPY_MODEL_c333be4f6eee4613b3774602694082a9",
            "value": "Loading weights: 100%"
          }
        },
        "8c30df575597433a95947c62c1542780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76bbeffe40e74ce2a395fefcb44aea95",
            "max": 590,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_491199489732400e97470a7ad5281b4e",
            "value": 590
          }
        },
        "a1ee093494a841bd950bffde7dddcbf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d00f1a86d8145bfba4bc617522717e3",
            "max": 590,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a7247a3dc29458bb5b64a2ac8349de8",
            "value": 590
          }
        },
        "ab56ae55efe746c7bcfc041e48bd1410": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8899531cfacd47dabf06c000625b126f",
              "IPY_MODEL_a1ee093494a841bd950bffde7dddcbf4",
              "IPY_MODEL_5069bc1529db46e9850320b73c7c1d9c"
            ],
            "layout": "IPY_MODEL_50bb2c8a422a44f59da31ff110b98793"
          }
        },
        "ba1e67e6529f4895961f6657f8c3ee91": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bff3ba233e494a389f439965f2f5ae5e",
              "IPY_MODEL_2df28c6655164aa09452657827a4a67c",
              "IPY_MODEL_6dd4c0ca40674edb8febfec250159b3f"
            ],
            "layout": "IPY_MODEL_fd2f5996a2714a789dc7f0e8df5c0f93"
          }
        },
        "bff3ba233e494a389f439965f2f5ae5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1d52121b02648bfa379bb5a6538a455",
            "placeholder": "​",
            "style": "IPY_MODEL_06c0f02297f545918bd87eab48d866f1",
            "value": "Loading weights: 100%"
          }
        },
        "c0593deb81674c96ad25ad7c481b5152": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa5802d6f6234af99090fb47aab34c03",
            "placeholder": "​",
            "style": "IPY_MODEL_c42ae601764a4ab2842e88faff232581",
            "value": "Loading weights: 100%"
          }
        },
        "c2c40671a93a4f3e8d085744423206cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c333be4f6eee4613b3774602694082a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c42ae601764a4ab2842e88faff232581": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e1d52121b02648bfa379bb5a6538a455": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa5802d6f6234af99090fb47aab34c03": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fceb0e1010a74a51862e7b7a2a8ebe9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd2f5996a2714a789dc7f0e8df5c0f93": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
